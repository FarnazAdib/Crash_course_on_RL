{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Quadratic control\n",
    "_Linear Quadratic (LQ)_ problem is a classical control problem where the dynamical system obeys linear dynamics and the cost function to be minimized is quadratic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics\n",
    "We consider a linear Gaussian dynamical system\n",
    "\n",
    "\\begin{align*}\n",
    "s_{k+1}=A s_{k}+B u_{k}+ w_{k},\n",
    "\\end{align*}\n",
    "\n",
    "where $s_{k} \\in \\mathbb{R}^{n}$ and $u_{k} \\in \\mathbb{R}^{m}$ are the state and the control input vectors respectively. The vector $w_{k} \\in \\mathbb{R}^{n}$ denotes the process noise drawn i.i.d. from a Gaussian distribution $\\mathcal{N}(\\textbf{0}, W_{w})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "We define the  _quadratic running cost_ as\n",
    "\n",
    "\\begin{align*}\n",
    "c_{k}=s_{k}^{T} Q s_{k}+u_{k}^T R u_{k}\n",
    "\\end{align*}\n",
    "\n",
    "where $Q \\geq 0$ and $R>0$ are the state and the control weighting matrices respectively. The _average cost associated with the policy_ $\\pi =K s_{k}$ is defined by\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda=\\lim_{\\tau \\rightarrow \\infty} \\frac{1}{\\tau} \\mathbf{E}[ \\sum_{t=1}^{\\tau} c_{t} ]\n",
    "\\end{align*}\n",
    "\n",
    "which does not depend on the initial state of the system. For the linear system, we define the _value function_ associated with a given policy $\\pi=K s_{k}$ as\n",
    "\n",
    "\\begin{align*}\n",
    "V(s_{k})=\\mathbf{E}[\\sum_{t=k}^{+\\infty} (c_{t}-\\lambda) | s_{k}].\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solvability Criterion\n",
    "We aim to find the policy $\\pi^{*}=K^{*} s_{k}$ to minimize $V(s_{k})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why the LQ is an interesting setup?\n",
    "But why do we consider to solve an LQ problem with RL when we can simply estimate the linear model?\n",
    "\n",
    "* The LQ problem has a celebrated closed-form solution. It is an ideal benchmark for studying the RL algorithms because we know the exact analytical solution so we can compare RL algorithms against the analytical solution and see how good they are.\n",
    "* It is theoretically tractable.\n",
    "* It is practical in various engineering domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning to Control a cartpole\n",
    "We apply two basic RL routines; namely _Policy Gradient (PG)_ and $Q$-_learning_ for the LQ problem. Here is a list of related files to study:\n",
    "\n",
    "* [Prepare a virtual environment.](Preparation.ipynb) If you have prepared the virtual environment previously, skip this step but remember to select it as the Kernel when running codes in Jupyter notebooks.\n",
    "* [Policy Gradient on LQ](pg_on_lq_notebook.ipynb)\n",
    "* [$Q$-learning on LQ](q_on_lq_notebook.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
