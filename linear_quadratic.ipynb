{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Quadratic control\n",
    "_Linear Quadratic (LQ)_ problem is a classical control problem where the dynamical system obeys linear dynamics and the cost function to be minimized is quadratic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics\n",
    "We consider a linear Gaussian dynamical system\n",
    "\n",
    "\\begin{align*}\n",
    "s_{t+1}=A s_{t}+B u_{t}+ w_{t},\n",
    "\\end{align*}\n",
    "\n",
    "where $s_{t} \\in \\mathbb{R}^{n}$ and $u_{t} \\in \\mathbb{R}^{m}$ are the state and the control input vectors respectively. The vector $w_{t} \\in \\mathbb{R}^{n}$ denotes the process noise drawn i.i.d. from a Gaussian distribution $\\mathcal{N}(\\textbf{0}, W_{w})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "We define the  _quadratic running cost_ as\n",
    "\n",
    "\\begin{align*}\n",
    "c_{t}=s_{t}^{\\dagger} Q s_{t}+u_{t}^{\\dagger} R u_{t}\n",
    "\\end{align*}\n",
    "\n",
    "where $Q \\geq 0$ and $R>0$ are the state and the control weighting matrices respectively and we have used $\\dagger$ to represent transpose. It is enough to consider the reward as $ r_t = - c_t$. In the literature related to LQ problem, it is common to use and derive equations based on cost rather than reward. So, here we follow the same trend as the literature and use $c_t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solvability Criterion\n",
    "Define the _value function_ associated with a given policy $\\pi=K s_{t}$ as\n",
    "\n",
    "\\begin{align*}\n",
    "V(s_{t})=\\mathbf{E}[\\sum_{k=t}^{+\\infty} (c_{k}-\\lambda) |s_{t}]=\\mathbf{E}[c_{t}-\\lambda + c_{t+1}- \\lambda + ... | s_{t}]\n",
    "\\end{align*}\n",
    "where $\\lambda$ is the _average cost associated with the policy_ $\\pi =K s_{t}$ \n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda=\\lim_{T \\rightarrow \\infty} \\frac{1}{T}  \\sum_{t=1}^{T} c_{t} .\n",
    "\\end{align*}\n",
    "\n",
    "We aim to find the policy $\\pi=K s_{t}$ to minimize $V(s_{t})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is the LQ problem an interesting setup?\n",
    "But why do we consider to solve an LQ problem with RL when we can simply estimate the linear model?\n",
    "\n",
    "* The LQ problem has a celebrated closed-form solution. It is an ideal benchmark for studying the RL algorithms because we know the exact analytical solution so we can compare RL algorithms against the analytical solution and see how good they are.\n",
    "* It is theoretically tractable.\n",
    "* It is practical in various engineering domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning to Control a cartpole\n",
    "We apply two basic RL routines; namely _Policy Gradient (PG)_ and $Q$-_learning_ for the LQ problem. Here is a list of related files to study:\n",
    "\n",
    "* [Prepare a virtual environment.](Preparation.ipynb) If you have prepared the virtual environment previously, skip this step but remember to select it as the Kernel when running codes in Jupyter notebooks.\n",
    "* [Policy Gradient on LQ](pg_on_lq_notebook.ipynb)\n",
    "* [$Q$-learning on LQ](q_on_lq_notebook.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
