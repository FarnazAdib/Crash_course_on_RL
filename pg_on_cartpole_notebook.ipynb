{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient on cartpole\n",
    "In this notebook, we run the policy gradient algorithm on the cartpole example. \n",
    "* [You can read about the cartpole problem here.](cartpole.ipynb)\n",
    "* [You can read about policy gradient here.](pg_notebook.ipynb)\n",
    "* [You can see the pure code for policy gradient on cartpole here.](./cartpole/pg_on_cartpole.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the algorithm\n",
    "We build a (deep) network to represent the probability density function $\\pi_\\theta$= `network(state).`\n",
    "\n",
    "\n",
    "```\n",
    "network = keras.Sequential([\n",
    "            keras.layers.Dense(30, input_dim=n_s, activation='relu'),\n",
    "            keras.layers.Dense(30, activation='relu'),\n",
    "            keras.layers.Dense(n_a, activation='softmax')])\n",
    "network.compile(loss='categorical_crossentropy')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we iteratively improve the network. In each iteration of the algorithm, we do the following\n",
    "\n",
    "* i. We rollout the environment to collect data for PG by following these steps:\n",
    "    * i.a. We initialize empty histories for `states=[]`, `actions=[]`, `rewards=[]`\n",
    "    * i.b. We observe the `state` $s$ and sample `action` $a$ from the poliy pdf $\\pi_{\\theta}(s)$\n",
    "    \n",
    "    `softmax_out = network(state)`\n",
    "    \n",
    "    `a = np.random.choice(n_a, p=softmax_out.numpy()[0])`\n",
    "    \n",
    "    * i.c. We derive the environment using $a$ and observe the `reward` $r$.\n",
    "    * i.d. We add $s,\\:a,\\:r$ to the history batch `states`, `actions`, `rewards`.\n",
    "    * i.e. We continue from i.b. until the episode ends.\n",
    "* ii. We improve the policy by following these steps\n",
    "    * ii.a. We calculate the reward to go and standardize it. \n",
    "    * ii.b. We optimize the policy.\n",
    "    \n",
    "    `\n",
    "    target_actions = tf.keras.utils.to_categorical(np.array(actions), n_a)\n",
    "    loss = self.network.train_on_batch(states, target_actions, sample_weight=rewards_to_go)\n",
    "    `\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on google colab\n",
    "If you want to run on google colab, go ahead and run the following cell. If you want to run on your computer, skip this cell and start from Importing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farad59\\Desktop\\Works\\Crash Course on RL\\Crash_course_on_RL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Crash_course_on_RL'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\farad59\\desktop\\works\\crash course on rl\\crash_course_on_rl\n",
      "Building wheels for collected packages: Reinforcement-Learning-for-Control\n",
      "  Running setup.py bdist_wheel for Reinforcement-Learning-for-Control: started\n",
      "  Running setup.py bdist_wheel for Reinforcement-Learning-for-Control: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\farad59\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-i910pr1q\\wheels\\3f\\f7\\0c\\571f7571248732281b3a24cb152bda36892110c103a1fd8f9d\n",
      "Successfully built Reinforcement-Learning-for-Control\n",
      "Installing collected packages: Reinforcement-Learning-for-Control\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow 2.3.0 requires astunparse==1.6.3, which is not installed.\n",
      "tensorflow 2.3.0 requires google-pasta>=0.1.8, which is not installed.\n",
      "tensorflow 2.3.0 requires opt-einsum>=2.3.2, which is not installed.\n",
      "tensorflow 2.3.0 requires protobuf>=3.9.2, which is not installed.\n",
      "tensorboard 2.3.0 requires google-auth<2,>=1.6.3, which is not installed.\n",
      "tensorboard 2.3.0 requires google-auth-oauthlib<0.5,>=0.4.1, which is not installed.\n",
      "tensorboard 2.3.0 requires protobuf>=3.6.0, which is not installed.\n",
      "tensorboard 2.3.0 requires tensorboard-plugin-wit>=1.6.0, which is not installed.\n",
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "tensorflow 2.3.0 has requirement h5py<2.11.0,>=2.10.0, but you'll have h5py 2.7.1 which is incompatible.\n",
      "tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.14.3 which is incompatible.\n",
      "tensorflow 2.3.0 has requirement scipy==1.4.1, but you'll have scipy 1.1.0 which is incompatible.\n",
      "tensorflow 2.3.0 has requirement six>=1.12.0, but you'll have six 1.11.0 which is incompatible.\n",
      "tensorboard 2.3.0 has requirement grpcio>=1.24.3, but you'll have grpcio 1.22.0 which is incompatible.\n",
      "tensorboard 2.3.0 has requirement requests<3,>=2.21.0, but you'll have requests 2.18.4 which is incompatible.\n",
      "tensorboard 2.3.0 has requirement setuptools>=41.0.0, but you'll have setuptools 39.1.0 which is incompatible.\n",
      "Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\Shared\\\\Anaconda3_86\\\\Lib\\\\site-packages\\\\Reinforcement_Learning_for_Control-0.0.1.dist-info'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "You are using pip version 10.0.1, however version 20.3b1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/FarnazAdib/Crash_course_on_RL.git\n",
    "%cd Crash_course_on_RL\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start coding by importing the required libraries. If you get an error, you have possibly forgotten to change the kernel. See [Prepare a virtual environment](Preparation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import datetime as dt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from cartpole.dynamics import CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving directories\n",
    "Next, we set up some paths to write data and possibly capture some videos for future investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORE_PATH = '/tmp/cartpole_exp1/PG'\n",
    "data_path = STORE_PATH + f\"/data_{dt.datetime.now().strftime('%d%m%Y%H%M')}\"\n",
    "agent_path = STORE_PATH + f\"/agent_{dt.datetime.now().strftime('%d%m%Y%H%M')}\"\n",
    "train_writer = tf.summary.create_file_writer(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the environment\n",
    "We select the random seed and make the cartpole environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rand_Seed = 1\n",
    "env_par = {\n",
    "    'Rand_Seed': Rand_Seed,\n",
    "    'STORE_PATH': STORE_PATH,\n",
    "    'monitor': False,\n",
    "    'threshold': 195.0\n",
    "}\n",
    "Rand_Seed = 1\n",
    "CP = CartPole(env_par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the policy gradient agent\n",
    "We define the policy gradient class. This class receives a dictionary with the following entries:\n",
    "* `hidden_size`: Number of nodes in the layers.\n",
    "* `GAMMA`: forgetting factor in the total cost. It should be in $[0\\:1]$.\n",
    "* `num_episodes`: The maximum number of episodes to run.\n",
    "* `learning_rate_adam`: The learning rate for adam optimization.\n",
    "* `adam_eps`: The epsilon in adam optimization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PG:\n",
    "    def __init__(self, hparams):\n",
    "        self.hparams = hparams\n",
    "        np.random.seed(hparams['Rand_Seed'])\n",
    "        tf.random.set_seed(hparams['Rand_Seed'])\n",
    "\n",
    "        # The policy network\n",
    "        self.network = keras.Sequential([\n",
    "            keras.layers.Dense(self.hparams['hidden_size'], input_dim=self.hparams['num_state'], activation='relu',\n",
    "                               kernel_initializer=keras.initializers.he_normal(), dtype='float64'),\n",
    "            keras.layers.Dense(self.hparams['hidden_size'], activation='relu',\n",
    "                               kernel_initializer=keras.initializers.he_normal(), dtype='float64'),\n",
    "            keras.layers.Dense(self.hparams['num_actions'], activation='softmax', dtype='float64')\n",
    "        ])\n",
    "        self.network.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(\n",
    "            epsilon=self.hparams['adam_eps'], learning_rate=self.hparams['learning_rate_adam']))\n",
    "\n",
    "    def get_action(self, state, env):\n",
    "        \n",
    "        # Building the pdf for the given state\n",
    "        softmax_out = self.network(state.reshape((1, -1)))\n",
    "        \n",
    "        # Sampling an action according to the pdf\n",
    "        selected_action = np.random.choice(self.hparams['num_actions'], p=softmax_out.numpy()[0])\n",
    "        return selected_action\n",
    "\n",
    "    def update_network(self, states, actions, rewards):\n",
    "        reward_sum = 0\n",
    "        rewards_to_go = []\n",
    "        for reward in rewards[::-1]:  # reverse buffer r\n",
    "            reward_sum = reward + self.hparams['GAMMA'] * reward_sum\n",
    "            rewards_to_go.append(reward_sum)\n",
    "        rewards_to_go.reverse()\n",
    "        rewards_to_go = np.array(rewards_to_go)\n",
    "        # standardise the rewards\n",
    "        rewards_to_go -= np.mean(rewards_to_go)\n",
    "        rewards_to_go /= np.std(rewards_to_go)\n",
    "        states = np.vstack(states)\n",
    "        target_actions = tf.keras.utils.to_categorical(np.array(actions), self.hparams['num_actions'])\n",
    "        loss = self.network.train_on_batch(states, target_actions, sample_weight=rewards_to_go)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have configured our network, by selecting the structure and cost function to be minmized. The last step is to feed the network with `states`, `actions`, `next_states`, and `dones` and update the parameters of the network. This is done by the function `update_network(self, states, actions, rewards, next_states, dones)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the $Q$-learning algorithm, it is enough to build an object and to iterate. You can change the following hyper parameters if you like\n",
    "\n",
    "* `hidden_size`: Number of nodes in the layers.\n",
    "* `GAMMA`: forgetting factor in the total cost. It should be in $[0\\:1]$.\n",
    "* `num_episodes`: The maximum number of episodes to run.\n",
    "* `learning_rate_adam`: The learning rate for adam optimization.\n",
    "* `adam_eps`: The epsilon in adam optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_par = {\n",
    "    'num_state': CP.env.observation_space.shape[0],\n",
    "    'num_actions': CP.env.action_space.n,\n",
    "    'Rand_Seed': Rand_Seed,\n",
    "    'hidden_size': 30,\n",
    "    'GAMMA': 1.0,\n",
    "    'num_episodes': 1000,\n",
    "    'learning_rate_adam': 0.001,\n",
    "    'adam_eps': 1e-7,\n",
    "}\n",
    "policy = PG(agent_par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start learning\n",
    "Now, we start the learning loop. The learning loop iterates for a maximum of number `num_episodes`. In each iteration\n",
    "* The agent derives the environment for one episode to collect data for PG.\n",
    "* We update the agent by policy gradient algorithm using the recorded data.\n",
    "* We check if the problem is solved.\n",
    "* We write the data.\n",
    "At the end of the learning loop, we close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Reward: 11.0, Mean of 100 cons episodes: 0\n",
      "Episode: 1, Reward: 28.0, Mean of 100 cons episodes: 0\n",
      "Episode: 2, Reward: 26.0, Mean of 100 cons episodes: 0\n",
      "Episode: 3, Reward: 19.0, Mean of 100 cons episodes: 0\n",
      "Episode: 4, Reward: 20.0, Mean of 100 cons episodes: 0\n",
      "Episode: 5, Reward: 14.0, Mean of 100 cons episodes: 0\n",
      "Episode: 6, Reward: 12.0, Mean of 100 cons episodes: 0\n",
      "Episode: 7, Reward: 12.0, Mean of 100 cons episodes: 0\n",
      "Episode: 8, Reward: 10.0, Mean of 100 cons episodes: 0\n",
      "Episode: 9, Reward: 37.0, Mean of 100 cons episodes: 0\n",
      "Episode: 10, Reward: 14.0, Mean of 100 cons episodes: 0\n",
      "Episode: 11, Reward: 42.0, Mean of 100 cons episodes: 0\n",
      "Episode: 12, Reward: 15.0, Mean of 100 cons episodes: 0\n",
      "Episode: 13, Reward: 10.0, Mean of 100 cons episodes: 0\n",
      "Episode: 14, Reward: 9.0, Mean of 100 cons episodes: 0\n",
      "Episode: 15, Reward: 11.0, Mean of 100 cons episodes: 0\n",
      "Episode: 16, Reward: 28.0, Mean of 100 cons episodes: 0\n",
      "Episode: 17, Reward: 14.0, Mean of 100 cons episodes: 0\n",
      "Episode: 18, Reward: 28.0, Mean of 100 cons episodes: 0\n",
      "Episode: 19, Reward: 15.0, Mean of 100 cons episodes: 0\n",
      "Episode: 20, Reward: 29.0, Mean of 100 cons episodes: 0\n",
      "Episode: 21, Reward: 11.0, Mean of 100 cons episodes: 0\n",
      "Episode: 22, Reward: 16.0, Mean of 100 cons episodes: 0\n",
      "Episode: 23, Reward: 11.0, Mean of 100 cons episodes: 0\n",
      "Episode: 24, Reward: 15.0, Mean of 100 cons episodes: 0\n",
      "Episode: 25, Reward: 10.0, Mean of 100 cons episodes: 0\n",
      "Episode: 26, Reward: 11.0, Mean of 100 cons episodes: 0\n",
      "Episode: 27, Reward: 12.0, Mean of 100 cons episodes: 0\n",
      "Episode: 28, Reward: 19.0, Mean of 100 cons episodes: 0\n",
      "Episode: 29, Reward: 9.0, Mean of 100 cons episodes: 0\n",
      "Episode: 30, Reward: 27.0, Mean of 100 cons episodes: 0\n",
      "Episode: 31, Reward: 15.0, Mean of 100 cons episodes: 0\n",
      "Episode: 32, Reward: 29.0, Mean of 100 cons episodes: 0\n",
      "Episode: 33, Reward: 17.0, Mean of 100 cons episodes: 0\n",
      "Episode: 34, Reward: 20.0, Mean of 100 cons episodes: 0\n",
      "Episode: 35, Reward: 37.0, Mean of 100 cons episodes: 0\n",
      "Episode: 36, Reward: 20.0, Mean of 100 cons episodes: 0\n",
      "Episode: 37, Reward: 39.0, Mean of 100 cons episodes: 0\n",
      "Episode: 38, Reward: 21.0, Mean of 100 cons episodes: 0\n",
      "Episode: 39, Reward: 29.0, Mean of 100 cons episodes: 0\n",
      "Episode: 40, Reward: 15.0, Mean of 100 cons episodes: 0\n",
      "Episode: 41, Reward: 20.0, Mean of 100 cons episodes: 0\n",
      "Episode: 42, Reward: 26.0, Mean of 100 cons episodes: 0\n",
      "Episode: 43, Reward: 10.0, Mean of 100 cons episodes: 0\n",
      "Episode: 44, Reward: 19.0, Mean of 100 cons episodes: 0\n",
      "Episode: 45, Reward: 30.0, Mean of 100 cons episodes: 0\n",
      "Episode: 46, Reward: 17.0, Mean of 100 cons episodes: 0\n",
      "Episode: 47, Reward: 24.0, Mean of 100 cons episodes: 0\n",
      "Episode: 48, Reward: 17.0, Mean of 100 cons episodes: 0\n",
      "Episode: 49, Reward: 10.0, Mean of 100 cons episodes: 0\n",
      "Episode: 50, Reward: 35.0, Mean of 100 cons episodes: 0\n",
      "Episode: 51, Reward: 26.0, Mean of 100 cons episodes: 0\n",
      "Episode: 52, Reward: 47.0, Mean of 100 cons episodes: 0\n",
      "Episode: 53, Reward: 27.0, Mean of 100 cons episodes: 0\n",
      "Episode: 54, Reward: 39.0, Mean of 100 cons episodes: 0\n",
      "Episode: 55, Reward: 17.0, Mean of 100 cons episodes: 0\n",
      "Episode: 56, Reward: 25.0, Mean of 100 cons episodes: 0\n",
      "Episode: 57, Reward: 18.0, Mean of 100 cons episodes: 0\n",
      "Episode: 58, Reward: 17.0, Mean of 100 cons episodes: 0\n",
      "Episode: 59, Reward: 13.0, Mean of 100 cons episodes: 0\n",
      "Episode: 60, Reward: 16.0, Mean of 100 cons episodes: 0\n",
      "Episode: 61, Reward: 15.0, Mean of 100 cons episodes: 0\n",
      "Episode: 62, Reward: 45.0, Mean of 100 cons episodes: 0\n",
      "Episode: 63, Reward: 28.0, Mean of 100 cons episodes: 0\n",
      "Episode: 64, Reward: 24.0, Mean of 100 cons episodes: 0\n",
      "Episode: 65, Reward: 20.0, Mean of 100 cons episodes: 0\n",
      "Episode: 66, Reward: 12.0, Mean of 100 cons episodes: 0\n",
      "Episode: 67, Reward: 16.0, Mean of 100 cons episodes: 0\n",
      "Episode: 68, Reward: 45.0, Mean of 100 cons episodes: 0\n",
      "Episode: 69, Reward: 15.0, Mean of 100 cons episodes: 0\n",
      "Episode: 70, Reward: 15.0, Mean of 100 cons episodes: 0\n",
      "Episode: 71, Reward: 28.0, Mean of 100 cons episodes: 0\n",
      "Episode: 72, Reward: 10.0, Mean of 100 cons episodes: 0\n",
      "Episode: 73, Reward: 17.0, Mean of 100 cons episodes: 0\n",
      "Episode: 74, Reward: 12.0, Mean of 100 cons episodes: 0\n",
      "Episode: 75, Reward: 19.0, Mean of 100 cons episodes: 0\n",
      "Episode: 76, Reward: 30.0, Mean of 100 cons episodes: 0\n",
      "Episode: 77, Reward: 18.0, Mean of 100 cons episodes: 0\n",
      "Episode: 78, Reward: 39.0, Mean of 100 cons episodes: 0\n",
      "Episode: 79, Reward: 36.0, Mean of 100 cons episodes: 0\n",
      "Episode: 80, Reward: 21.0, Mean of 100 cons episodes: 0\n",
      "Episode: 81, Reward: 12.0, Mean of 100 cons episodes: 0\n",
      "Episode: 82, Reward: 67.0, Mean of 100 cons episodes: 0\n",
      "Episode: 83, Reward: 12.0, Mean of 100 cons episodes: 0\n",
      "Episode: 84, Reward: 52.0, Mean of 100 cons episodes: 0\n",
      "Episode: 85, Reward: 29.0, Mean of 100 cons episodes: 0\n",
      "Episode: 86, Reward: 15.0, Mean of 100 cons episodes: 0\n",
      "Episode: 87, Reward: 17.0, Mean of 100 cons episodes: 0\n",
      "Episode: 88, Reward: 11.0, Mean of 100 cons episodes: 0\n",
      "Episode: 89, Reward: 39.0, Mean of 100 cons episodes: 0\n",
      "Episode: 90, Reward: 31.0, Mean of 100 cons episodes: 0\n",
      "Episode: 91, Reward: 23.0, Mean of 100 cons episodes: 0\n",
      "Episode: 92, Reward: 17.0, Mean of 100 cons episodes: 0\n",
      "Episode: 93, Reward: 26.0, Mean of 100 cons episodes: 0\n",
      "Episode: 94, Reward: 17.0, Mean of 100 cons episodes: 0\n",
      "Episode: 95, Reward: 10.0, Mean of 100 cons episodes: 0\n",
      "Episode: 96, Reward: 18.0, Mean of 100 cons episodes: 0\n",
      "Episode: 97, Reward: 25.0, Mean of 100 cons episodes: 0\n",
      "Episode: 98, Reward: 40.0, Mean of 100 cons episodes: 0\n",
      "Episode: 99, Reward: 29.0, Mean of 100 cons episodes: 0\n",
      "Episode: 100, Reward: 30.0, Mean of 100 cons episodes: 0\n",
      "Episode: 101, Reward: 23.0, Mean of 100 cons episodes: 21.95\n",
      "Episode: 102, Reward: 14.0, Mean of 100 cons episodes: 22.14\n",
      "Episode: 103, Reward: 26.0, Mean of 100 cons episodes: 22.09\n",
      "Episode: 104, Reward: 51.0, Mean of 100 cons episodes: 21.97\n",
      "Episode: 105, Reward: 45.0, Mean of 100 cons episodes: 22.04\n",
      "Episode: 106, Reward: 15.0, Mean of 100 cons episodes: 22.35\n",
      "Episode: 107, Reward: 28.0, Mean of 100 cons episodes: 22.66\n",
      "Episode: 108, Reward: 41.0, Mean of 100 cons episodes: 22.69\n",
      "Episode: 109, Reward: 26.0, Mean of 100 cons episodes: 22.85\n",
      "Episode: 110, Reward: 21.0, Mean of 100 cons episodes: 23.16\n",
      "Episode: 111, Reward: 28.0, Mean of 100 cons episodes: 23.05\n",
      "Episode: 112, Reward: 19.0, Mean of 100 cons episodes: 23.12\n",
      "Episode: 113, Reward: 13.0, Mean of 100 cons episodes: 22.98\n",
      "Episode: 114, Reward: 30.0, Mean of 100 cons episodes: 23.02\n",
      "Episode: 115, Reward: 29.0, Mean of 100 cons episodes: 23.05\n",
      "Episode: 116, Reward: 34.0, Mean of 100 cons episodes: 23.26\n",
      "Episode: 117, Reward: 15.0, Mean of 100 cons episodes: 23.44\n",
      "Episode: 118, Reward: 23.0, Mean of 100 cons episodes: 23.5\n",
      "Episode: 119, Reward: 37.0, Mean of 100 cons episodes: 23.51\n",
      "Episode: 120, Reward: 13.0, Mean of 100 cons episodes: 23.46\n",
      "Episode: 121, Reward: 20.0, Mean of 100 cons episodes: 23.68\n",
      "Episode: 122, Reward: 18.0, Mean of 100 cons episodes: 23.52\n",
      "Episode: 123, Reward: 29.0, Mean of 100 cons episodes: 23.61\n",
      "Episode: 124, Reward: 24.0, Mean of 100 cons episodes: 23.63\n",
      "Episode: 125, Reward: 41.0, Mean of 100 cons episodes: 23.81\n",
      "Episode: 126, Reward: 13.0, Mean of 100 cons episodes: 23.9\n",
      "Episode: 127, Reward: 62.0, Mean of 100 cons episodes: 24.21\n",
      "Episode: 128, Reward: 49.0, Mean of 100 cons episodes: 24.23\n",
      "Episode: 129, Reward: 27.0, Mean of 100 cons episodes: 24.73\n",
      "Episode: 130, Reward: 35.0, Mean of 100 cons episodes: 25.03\n",
      "Episode: 131, Reward: 35.0, Mean of 100 cons episodes: 25.21\n",
      "Episode: 132, Reward: 29.0, Mean of 100 cons episodes: 25.29\n",
      "Episode: 133, Reward: 64.0, Mean of 100 cons episodes: 25.49\n",
      "Episode: 134, Reward: 18.0, Mean of 100 cons episodes: 25.49\n",
      "Episode: 135, Reward: 24.0, Mean of 100 cons episodes: 25.96\n",
      "Episode: 136, Reward: 31.0, Mean of 100 cons episodes: 25.94\n",
      "Episode: 137, Reward: 57.0, Mean of 100 cons episodes: 25.81\n",
      "Episode: 138, Reward: 47.0, Mean of 100 cons episodes: 25.92\n",
      "Episode: 139, Reward: 15.0, Mean of 100 cons episodes: 26.1\n",
      "Episode: 140, Reward: 19.0, Mean of 100 cons episodes: 26.36\n",
      "Episode: 141, Reward: 33.0, Mean of 100 cons episodes: 26.22\n",
      "Episode: 142, Reward: 39.0, Mean of 100 cons episodes: 26.26\n",
      "Episode: 143, Reward: 32.0, Mean of 100 cons episodes: 26.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 144, Reward: 30.0, Mean of 100 cons episodes: 26.52\n",
      "Episode: 145, Reward: 22.0, Mean of 100 cons episodes: 26.74\n",
      "Episode: 146, Reward: 40.0, Mean of 100 cons episodes: 26.85\n",
      "Episode: 147, Reward: 28.0, Mean of 100 cons episodes: 26.77\n",
      "Episode: 148, Reward: 59.0, Mean of 100 cons episodes: 27.0\n",
      "Episode: 149, Reward: 25.0, Mean of 100 cons episodes: 27.04\n",
      "Episode: 150, Reward: 33.0, Mean of 100 cons episodes: 27.46\n",
      "Episode: 151, Reward: 33.0, Mean of 100 cons episodes: 27.61\n",
      "Episode: 152, Reward: 50.0, Mean of 100 cons episodes: 27.59\n",
      "Episode: 153, Reward: 24.0, Mean of 100 cons episodes: 27.66\n",
      "Episode: 154, Reward: 25.0, Mean of 100 cons episodes: 27.69\n",
      "Episode: 155, Reward: 15.0, Mean of 100 cons episodes: 27.66\n",
      "Episode: 156, Reward: 57.0, Mean of 100 cons episodes: 27.52\n",
      "Episode: 157, Reward: 70.0, Mean of 100 cons episodes: 27.5\n",
      "Episode: 158, Reward: 86.0, Mean of 100 cons episodes: 27.82\n",
      "Episode: 159, Reward: 16.0, Mean of 100 cons episodes: 28.34\n",
      "Episode: 160, Reward: 47.0, Mean of 100 cons episodes: 29.03\n",
      "Episode: 161, Reward: 30.0, Mean of 100 cons episodes: 29.06\n",
      "Episode: 162, Reward: 14.0, Mean of 100 cons episodes: 29.37\n",
      "Episode: 163, Reward: 40.0, Mean of 100 cons episodes: 29.52\n",
      "Episode: 164, Reward: 29.0, Mean of 100 cons episodes: 29.21\n",
      "Episode: 165, Reward: 53.0, Mean of 100 cons episodes: 29.33\n",
      "Episode: 166, Reward: 15.0, Mean of 100 cons episodes: 29.38\n",
      "Episode: 167, Reward: 53.0, Mean of 100 cons episodes: 29.71\n",
      "Episode: 168, Reward: 22.0, Mean of 100 cons episodes: 29.74\n",
      "Episode: 169, Reward: 14.0, Mean of 100 cons episodes: 30.11\n",
      "Episode: 170, Reward: 43.0, Mean of 100 cons episodes: 29.88\n",
      "Episode: 171, Reward: 20.0, Mean of 100 cons episodes: 29.87\n",
      "Episode: 172, Reward: 15.0, Mean of 100 cons episodes: 30.15\n",
      "Episode: 173, Reward: 36.0, Mean of 100 cons episodes: 30.07\n",
      "Episode: 174, Reward: 65.0, Mean of 100 cons episodes: 30.12\n",
      "Episode: 175, Reward: 79.0, Mean of 100 cons episodes: 30.31\n",
      "Episode: 176, Reward: 98.0, Mean of 100 cons episodes: 30.84\n",
      "Episode: 177, Reward: 39.0, Mean of 100 cons episodes: 31.44\n",
      "Episode: 178, Reward: 22.0, Mean of 100 cons episodes: 32.12\n",
      "Episode: 179, Reward: 40.0, Mean of 100 cons episodes: 32.33\n",
      "Episode: 180, Reward: 57.0, Mean of 100 cons episodes: 32.16\n",
      "Episode: 181, Reward: 70.0, Mean of 100 cons episodes: 32.2\n",
      "Episode: 182, Reward: 29.0, Mean of 100 cons episodes: 32.56\n",
      "Episode: 183, Reward: 53.0, Mean of 100 cons episodes: 33.14\n",
      "Episode: 184, Reward: 89.0, Mean of 100 cons episodes: 32.76\n",
      "Episode: 185, Reward: 34.0, Mean of 100 cons episodes: 33.17\n",
      "Episode: 186, Reward: 32.0, Mean of 100 cons episodes: 33.54\n",
      "Episode: 187, Reward: 23.0, Mean of 100 cons episodes: 33.59\n",
      "Episode: 188, Reward: 75.0, Mean of 100 cons episodes: 33.76\n",
      "Episode: 189, Reward: 21.0, Mean of 100 cons episodes: 33.82\n",
      "Episode: 190, Reward: 77.0, Mean of 100 cons episodes: 34.46\n",
      "Episode: 191, Reward: 15.0, Mean of 100 cons episodes: 34.28\n",
      "Episode: 192, Reward: 82.0, Mean of 100 cons episodes: 34.74\n",
      "Episode: 193, Reward: 38.0, Mean of 100 cons episodes: 34.66\n",
      "Episode: 194, Reward: 43.0, Mean of 100 cons episodes: 35.31\n",
      "Episode: 195, Reward: 53.0, Mean of 100 cons episodes: 35.43\n",
      "Episode: 196, Reward: 45.0, Mean of 100 cons episodes: 35.69\n",
      "Episode: 197, Reward: 56.0, Mean of 100 cons episodes: 36.12\n",
      "Episode: 198, Reward: 77.0, Mean of 100 cons episodes: 36.39\n",
      "Episode: 199, Reward: 48.0, Mean of 100 cons episodes: 36.7\n",
      "Episode: 200, Reward: 19.0, Mean of 100 cons episodes: 37.07\n",
      "Episode: 201, Reward: 55.0, Mean of 100 cons episodes: 37.26\n",
      "Episode: 202, Reward: 90.0, Mean of 100 cons episodes: 37.15\n",
      "Episode: 203, Reward: 28.0, Mean of 100 cons episodes: 37.47\n",
      "Episode: 204, Reward: 33.0, Mean of 100 cons episodes: 38.23\n",
      "Episode: 205, Reward: 26.0, Mean of 100 cons episodes: 38.25\n",
      "Episode: 206, Reward: 63.0, Mean of 100 cons episodes: 38.07\n",
      "Episode: 207, Reward: 47.0, Mean of 100 cons episodes: 37.88\n",
      "Episode: 208, Reward: 27.0, Mean of 100 cons episodes: 38.36\n",
      "Episode: 209, Reward: 19.0, Mean of 100 cons episodes: 38.55\n",
      "Episode: 210, Reward: 30.0, Mean of 100 cons episodes: 38.41\n",
      "Episode: 211, Reward: 52.0, Mean of 100 cons episodes: 38.34\n",
      "Episode: 212, Reward: 18.0, Mean of 100 cons episodes: 38.43\n",
      "Episode: 213, Reward: 30.0, Mean of 100 cons episodes: 38.67\n",
      "Episode: 214, Reward: 20.0, Mean of 100 cons episodes: 38.66\n",
      "Episode: 215, Reward: 53.0, Mean of 100 cons episodes: 38.83\n",
      "Episode: 216, Reward: 57.0, Mean of 100 cons episodes: 38.73\n",
      "Episode: 217, Reward: 30.0, Mean of 100 cons episodes: 38.97\n",
      "Episode: 218, Reward: 32.0, Mean of 100 cons episodes: 39.2\n",
      "Episode: 219, Reward: 72.0, Mean of 100 cons episodes: 39.35\n",
      "Episode: 220, Reward: 28.0, Mean of 100 cons episodes: 39.44\n",
      "Episode: 221, Reward: 43.0, Mean of 100 cons episodes: 39.79\n",
      "Episode: 222, Reward: 62.0, Mean of 100 cons episodes: 39.94\n",
      "Episode: 223, Reward: 27.0, Mean of 100 cons episodes: 40.17\n",
      "Episode: 224, Reward: 39.0, Mean of 100 cons episodes: 40.61\n",
      "Episode: 225, Reward: 89.0, Mean of 100 cons episodes: 40.59\n",
      "Episode: 226, Reward: 55.0, Mean of 100 cons episodes: 40.74\n",
      "Episode: 227, Reward: 36.0, Mean of 100 cons episodes: 41.22\n",
      "Episode: 228, Reward: 39.0, Mean of 100 cons episodes: 41.64\n",
      "Episode: 229, Reward: 23.0, Mean of 100 cons episodes: 41.38\n",
      "Episode: 230, Reward: 34.0, Mean of 100 cons episodes: 41.28\n",
      "Episode: 231, Reward: 30.0, Mean of 100 cons episodes: 41.24\n",
      "Episode: 232, Reward: 30.0, Mean of 100 cons episodes: 41.23\n",
      "Episode: 233, Reward: 130.0, Mean of 100 cons episodes: 41.18\n",
      "Episode: 234, Reward: 50.0, Mean of 100 cons episodes: 41.19\n",
      "Episode: 235, Reward: 41.0, Mean of 100 cons episodes: 41.85\n",
      "Episode: 236, Reward: 38.0, Mean of 100 cons episodes: 42.17\n",
      "Episode: 237, Reward: 37.0, Mean of 100 cons episodes: 42.34\n",
      "Episode: 238, Reward: 66.0, Mean of 100 cons episodes: 42.41\n",
      "Episode: 239, Reward: 55.0, Mean of 100 cons episodes: 42.21\n",
      "Episode: 240, Reward: 78.0, Mean of 100 cons episodes: 42.4\n",
      "Episode: 241, Reward: 41.0, Mean of 100 cons episodes: 42.8\n",
      "Episode: 242, Reward: 90.0, Mean of 100 cons episodes: 43.39\n",
      "Episode: 243, Reward: 36.0, Mean of 100 cons episodes: 43.47\n",
      "Episode: 244, Reward: 53.0, Mean of 100 cons episodes: 43.98\n",
      "Episode: 245, Reward: 29.0, Mean of 100 cons episodes: 44.02\n",
      "Episode: 246, Reward: 72.0, Mean of 100 cons episodes: 44.25\n",
      "Episode: 247, Reward: 21.0, Mean of 100 cons episodes: 44.32\n",
      "Episode: 248, Reward: 167.0, Mean of 100 cons episodes: 44.64\n",
      "Episode: 249, Reward: 27.0, Mean of 100 cons episodes: 44.57\n",
      "Episode: 250, Reward: 38.0, Mean of 100 cons episodes: 45.65\n",
      "Episode: 251, Reward: 48.0, Mean of 100 cons episodes: 45.67\n",
      "Episode: 252, Reward: 56.0, Mean of 100 cons episodes: 45.72\n",
      "Episode: 253, Reward: 33.0, Mean of 100 cons episodes: 45.87\n",
      "Episode: 254, Reward: 40.0, Mean of 100 cons episodes: 45.93\n",
      "Episode: 255, Reward: 71.0, Mean of 100 cons episodes: 46.02\n",
      "Episode: 256, Reward: 71.0, Mean of 100 cons episodes: 46.17\n",
      "Episode: 257, Reward: 29.0, Mean of 100 cons episodes: 46.73\n",
      "Episode: 258, Reward: 29.0, Mean of 100 cons episodes: 46.87\n",
      "Episode: 259, Reward: 45.0, Mean of 100 cons episodes: 46.46\n",
      "Episode: 260, Reward: 82.0, Mean of 100 cons episodes: 45.89\n",
      "Episode: 261, Reward: 36.0, Mean of 100 cons episodes: 46.18\n",
      "Episode: 262, Reward: 35.0, Mean of 100 cons episodes: 46.53\n",
      "Episode: 263, Reward: 37.0, Mean of 100 cons episodes: 46.59\n",
      "Episode: 264, Reward: 67.0, Mean of 100 cons episodes: 46.8\n",
      "Episode: 265, Reward: 74.0, Mean of 100 cons episodes: 46.77\n",
      "Episode: 266, Reward: 158.0, Mean of 100 cons episodes: 47.15\n",
      "Episode: 267, Reward: 93.0, Mean of 100 cons episodes: 47.36\n",
      "Episode: 268, Reward: 99.0, Mean of 100 cons episodes: 48.79\n",
      "Episode: 269, Reward: 33.0, Mean of 100 cons episodes: 49.19\n",
      "Episode: 270, Reward: 67.0, Mean of 100 cons episodes: 49.96\n",
      "Episode: 271, Reward: 83.0, Mean of 100 cons episodes: 50.15\n",
      "Episode: 272, Reward: 39.0, Mean of 100 cons episodes: 50.39\n",
      "Episode: 273, Reward: 70.0, Mean of 100 cons episodes: 51.02\n",
      "Episode: 274, Reward: 36.0, Mean of 100 cons episodes: 51.26\n",
      "Episode: 275, Reward: 39.0, Mean of 100 cons episodes: 51.6\n",
      "Episode: 276, Reward: 57.0, Mean of 100 cons episodes: 51.31\n",
      "Episode: 277, Reward: 28.0, Mean of 100 cons episodes: 50.91\n",
      "Episode: 278, Reward: 40.0, Mean of 100 cons episodes: 50.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 279, Reward: 200.0, Mean of 100 cons episodes: 50.39\n",
      "Episode: 280, Reward: 79.0, Mean of 100 cons episodes: 50.57\n",
      "Episode: 281, Reward: 94.0, Mean of 100 cons episodes: 52.17\n",
      "Episode: 282, Reward: 99.0, Mean of 100 cons episodes: 52.39\n",
      "Episode: 283, Reward: 103.0, Mean of 100 cons episodes: 52.63\n",
      "Episode: 284, Reward: 125.0, Mean of 100 cons episodes: 53.33\n",
      "Episode: 285, Reward: 47.0, Mean of 100 cons episodes: 53.83\n",
      "Episode: 286, Reward: 65.0, Mean of 100 cons episodes: 54.19\n",
      "Episode: 287, Reward: 32.0, Mean of 100 cons episodes: 54.32\n",
      "Episode: 288, Reward: 128.0, Mean of 100 cons episodes: 54.65\n",
      "Episode: 289, Reward: 61.0, Mean of 100 cons episodes: 54.74\n",
      "Episode: 290, Reward: 37.0, Mean of 100 cons episodes: 55.27\n",
      "Episode: 291, Reward: 38.0, Mean of 100 cons episodes: 55.67\n",
      "Episode: 292, Reward: 39.0, Mean of 100 cons episodes: 55.27\n",
      "Episode: 293, Reward: 96.0, Mean of 100 cons episodes: 55.5\n",
      "Episode: 294, Reward: 64.0, Mean of 100 cons episodes: 55.07\n",
      "Episode: 295, Reward: 33.0, Mean of 100 cons episodes: 55.65\n",
      "Episode: 296, Reward: 67.0, Mean of 100 cons episodes: 55.86\n",
      "Episode: 297, Reward: 61.0, Mean of 100 cons episodes: 55.66\n",
      "Episode: 298, Reward: 104.0, Mean of 100 cons episodes: 55.88\n",
      "Episode: 299, Reward: 71.0, Mean of 100 cons episodes: 55.93\n",
      "Episode: 300, Reward: 116.0, Mean of 100 cons episodes: 56.2\n",
      "Episode: 301, Reward: 71.0, Mean of 100 cons episodes: 56.43\n",
      "Episode: 302, Reward: 39.0, Mean of 100 cons episodes: 57.4\n",
      "Episode: 303, Reward: 78.0, Mean of 100 cons episodes: 57.56\n",
      "Episode: 304, Reward: 138.0, Mean of 100 cons episodes: 57.05\n",
      "Episode: 305, Reward: 111.0, Mean of 100 cons episodes: 57.55\n",
      "Episode: 306, Reward: 85.0, Mean of 100 cons episodes: 58.6\n",
      "Episode: 307, Reward: 67.0, Mean of 100 cons episodes: 59.45\n",
      "Episode: 308, Reward: 95.0, Mean of 100 cons episodes: 59.67\n",
      "Episode: 309, Reward: 61.0, Mean of 100 cons episodes: 59.87\n",
      "Episode: 310, Reward: 104.0, Mean of 100 cons episodes: 60.55\n",
      "Episode: 311, Reward: 95.0, Mean of 100 cons episodes: 60.97\n",
      "Episode: 312, Reward: 74.0, Mean of 100 cons episodes: 61.71\n",
      "Episode: 313, Reward: 33.0, Mean of 100 cons episodes: 62.14\n",
      "Episode: 314, Reward: 190.0, Mean of 100 cons episodes: 62.7\n",
      "Episode: 315, Reward: 114.0, Mean of 100 cons episodes: 62.73\n",
      "Episode: 316, Reward: 74.0, Mean of 100 cons episodes: 64.43\n",
      "Episode: 317, Reward: 44.0, Mean of 100 cons episodes: 65.04\n",
      "Episode: 318, Reward: 60.0, Mean of 100 cons episodes: 65.21\n",
      "Episode: 319, Reward: 43.0, Mean of 100 cons episodes: 65.35\n",
      "Episode: 320, Reward: 176.0, Mean of 100 cons episodes: 65.63\n",
      "Episode: 321, Reward: 109.0, Mean of 100 cons episodes: 65.34\n",
      "Episode: 322, Reward: 38.0, Mean of 100 cons episodes: 66.82\n",
      "Episode: 323, Reward: 143.0, Mean of 100 cons episodes: 67.48\n",
      "Episode: 324, Reward: 109.0, Mean of 100 cons episodes: 67.24\n",
      "Episode: 325, Reward: 143.0, Mean of 100 cons episodes: 68.4\n",
      "Episode: 326, Reward: 84.0, Mean of 100 cons episodes: 69.1\n",
      "Episode: 327, Reward: 132.0, Mean of 100 cons episodes: 69.64\n",
      "Episode: 328, Reward: 67.0, Mean of 100 cons episodes: 69.93\n",
      "Episode: 329, Reward: 96.0, Mean of 100 cons episodes: 70.89\n",
      "Episode: 330, Reward: 82.0, Mean of 100 cons episodes: 71.17\n",
      "Episode: 331, Reward: 84.0, Mean of 100 cons episodes: 71.9\n",
      "Episode: 332, Reward: 159.0, Mean of 100 cons episodes: 72.38\n",
      "Episode: 333, Reward: 104.0, Mean of 100 cons episodes: 72.92\n",
      "Episode: 334, Reward: 85.0, Mean of 100 cons episodes: 74.21\n",
      "Episode: 335, Reward: 192.0, Mean of 100 cons episodes: 73.95\n",
      "Episode: 336, Reward: 76.0, Mean of 100 cons episodes: 74.3\n",
      "Episode: 337, Reward: 56.0, Mean of 100 cons episodes: 75.81\n",
      "Episode: 338, Reward: 111.0, Mean of 100 cons episodes: 76.19\n",
      "Episode: 339, Reward: 99.0, Mean of 100 cons episodes: 76.38\n",
      "Episode: 340, Reward: 123.0, Mean of 100 cons episodes: 76.83\n",
      "Episode: 341, Reward: 23.0, Mean of 100 cons episodes: 77.27\n",
      "Episode: 342, Reward: 42.0, Mean of 100 cons episodes: 77.72\n",
      "Episode: 343, Reward: 103.0, Mean of 100 cons episodes: 77.54\n",
      "Episode: 344, Reward: 109.0, Mean of 100 cons episodes: 77.06\n",
      "Episode: 345, Reward: 85.0, Mean of 100 cons episodes: 77.73\n",
      "Episode: 346, Reward: 17.0, Mean of 100 cons episodes: 78.29\n",
      "Episode: 347, Reward: 57.0, Mean of 100 cons episodes: 78.85\n",
      "Episode: 348, Reward: 60.0, Mean of 100 cons episodes: 78.3\n",
      "Episode: 349, Reward: 31.0, Mean of 100 cons episodes: 78.66\n",
      "Episode: 350, Reward: 98.0, Mean of 100 cons episodes: 77.59\n",
      "Episode: 351, Reward: 134.0, Mean of 100 cons episodes: 77.63\n",
      "Episode: 352, Reward: 117.0, Mean of 100 cons episodes: 78.23\n",
      "Episode: 353, Reward: 101.0, Mean of 100 cons episodes: 79.09\n",
      "Episode: 354, Reward: 200.0, Mean of 100 cons episodes: 79.7\n",
      "Episode: 355, Reward: 108.0, Mean of 100 cons episodes: 80.38\n",
      "Episode: 356, Reward: 91.0, Mean of 100 cons episodes: 81.98\n",
      "Episode: 357, Reward: 113.0, Mean of 100 cons episodes: 82.35\n",
      "Episode: 358, Reward: 30.0, Mean of 100 cons episodes: 82.55\n",
      "Episode: 359, Reward: 69.0, Mean of 100 cons episodes: 83.39\n",
      "Episode: 360, Reward: 99.0, Mean of 100 cons episodes: 83.4\n",
      "Episode: 361, Reward: 200.0, Mean of 100 cons episodes: 83.64\n",
      "Episode: 362, Reward: 92.0, Mean of 100 cons episodes: 83.81\n",
      "Episode: 363, Reward: 113.0, Mean of 100 cons episodes: 85.45\n",
      "Episode: 364, Reward: 121.0, Mean of 100 cons episodes: 86.02\n",
      "Episode: 365, Reward: 78.0, Mean of 100 cons episodes: 86.78\n",
      "Episode: 366, Reward: 63.0, Mean of 100 cons episodes: 87.32\n",
      "Episode: 367, Reward: 157.0, Mean of 100 cons episodes: 87.36\n",
      "Episode: 368, Reward: 86.0, Mean of 100 cons episodes: 86.41\n",
      "Episode: 369, Reward: 161.0, Mean of 100 cons episodes: 87.05\n",
      "Episode: 370, Reward: 96.0, Mean of 100 cons episodes: 86.92\n",
      "Episode: 371, Reward: 33.0, Mean of 100 cons episodes: 88.2\n",
      "Episode: 372, Reward: 117.0, Mean of 100 cons episodes: 88.49\n",
      "Episode: 373, Reward: 161.0, Mean of 100 cons episodes: 87.99\n",
      "Episode: 374, Reward: 122.0, Mean of 100 cons episodes: 88.77\n",
      "Episode: 375, Reward: 101.0, Mean of 100 cons episodes: 89.68\n",
      "Episode: 376, Reward: 59.0, Mean of 100 cons episodes: 90.54\n",
      "Episode: 377, Reward: 175.0, Mean of 100 cons episodes: 91.16\n",
      "Episode: 378, Reward: 96.0, Mean of 100 cons episodes: 91.18\n",
      "Episode: 379, Reward: 56.0, Mean of 100 cons episodes: 92.65\n",
      "Episode: 380, Reward: 100.0, Mean of 100 cons episodes: 93.21\n",
      "Episode: 381, Reward: 114.0, Mean of 100 cons episodes: 91.77\n",
      "Episode: 382, Reward: 123.0, Mean of 100 cons episodes: 91.98\n",
      "Episode: 383, Reward: 132.0, Mean of 100 cons episodes: 92.18\n",
      "Episode: 384, Reward: 121.0, Mean of 100 cons episodes: 92.42\n",
      "Episode: 385, Reward: 154.0, Mean of 100 cons episodes: 92.71\n",
      "Episode: 386, Reward: 104.0, Mean of 100 cons episodes: 92.67\n",
      "Episode: 387, Reward: 147.0, Mean of 100 cons episodes: 93.74\n",
      "Episode: 388, Reward: 200.0, Mean of 100 cons episodes: 94.13\n",
      "Episode: 389, Reward: 114.0, Mean of 100 cons episodes: 95.28\n",
      "Episode: 390, Reward: 106.0, Mean of 100 cons episodes: 96.0\n",
      "Episode: 391, Reward: 200.0, Mean of 100 cons episodes: 96.53\n",
      "Episode: 392, Reward: 70.0, Mean of 100 cons episodes: 97.22\n",
      "Episode: 393, Reward: 200.0, Mean of 100 cons episodes: 98.84\n",
      "Episode: 394, Reward: 86.0, Mean of 100 cons episodes: 99.15\n",
      "Episode: 395, Reward: 103.0, Mean of 100 cons episodes: 100.19\n",
      "Episode: 396, Reward: 89.0, Mean of 100 cons episodes: 100.41\n",
      "Episode: 397, Reward: 51.0, Mean of 100 cons episodes: 101.11\n",
      "Episode: 398, Reward: 148.0, Mean of 100 cons episodes: 101.33\n",
      "Episode: 399, Reward: 152.0, Mean of 100 cons episodes: 101.23\n",
      "Episode: 400, Reward: 118.0, Mean of 100 cons episodes: 101.67\n",
      "Episode: 401, Reward: 59.0, Mean of 100 cons episodes: 102.48\n",
      "Episode: 402, Reward: 85.0, Mean of 100 cons episodes: 102.5\n",
      "Episode: 403, Reward: 146.0, Mean of 100 cons episodes: 102.38\n",
      "Episode: 404, Reward: 163.0, Mean of 100 cons episodes: 102.84\n",
      "Episode: 405, Reward: 125.0, Mean of 100 cons episodes: 103.52\n",
      "Episode: 406, Reward: 124.0, Mean of 100 cons episodes: 103.77\n",
      "Episode: 407, Reward: 165.0, Mean of 100 cons episodes: 103.91\n",
      "Episode: 408, Reward: 94.0, Mean of 100 cons episodes: 104.3\n",
      "Episode: 409, Reward: 108.0, Mean of 100 cons episodes: 105.28\n",
      "Episode: 410, Reward: 84.0, Mean of 100 cons episodes: 105.27\n",
      "Episode: 411, Reward: 125.0, Mean of 100 cons episodes: 105.74\n",
      "Episode: 412, Reward: 200.0, Mean of 100 cons episodes: 105.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 413, Reward: 102.0, Mean of 100 cons episodes: 105.84\n",
      "Episode: 414, Reward: 55.0, Mean of 100 cons episodes: 107.1\n",
      "Episode: 415, Reward: 175.0, Mean of 100 cons episodes: 107.79\n",
      "Episode: 416, Reward: 85.0, Mean of 100 cons episodes: 106.44\n",
      "Episode: 417, Reward: 110.0, Mean of 100 cons episodes: 107.05\n",
      "Episode: 418, Reward: 101.0, Mean of 100 cons episodes: 107.16\n",
      "Episode: 419, Reward: 117.0, Mean of 100 cons episodes: 107.82\n",
      "Episode: 420, Reward: 86.0, Mean of 100 cons episodes: 108.23\n",
      "Episode: 421, Reward: 158.0, Mean of 100 cons episodes: 108.97\n",
      "Episode: 422, Reward: 124.0, Mean of 100 cons episodes: 108.07\n",
      "Episode: 423, Reward: 115.0, Mean of 100 cons episodes: 108.56\n",
      "Episode: 424, Reward: 66.0, Mean of 100 cons episodes: 109.42\n",
      "Episode: 425, Reward: 119.0, Mean of 100 cons episodes: 109.14\n",
      "Episode: 426, Reward: 122.0, Mean of 100 cons episodes: 108.71\n",
      "Episode: 427, Reward: 200.0, Mean of 100 cons episodes: 108.47\n",
      "Episode: 428, Reward: 106.0, Mean of 100 cons episodes: 108.85\n",
      "Episode: 429, Reward: 95.0, Mean of 100 cons episodes: 109.53\n",
      "Episode: 430, Reward: 168.0, Mean of 100 cons episodes: 109.92\n",
      "Episode: 431, Reward: 57.0, Mean of 100 cons episodes: 109.91\n",
      "Episode: 432, Reward: 200.0, Mean of 100 cons episodes: 110.77\n",
      "Episode: 433, Reward: 200.0, Mean of 100 cons episodes: 110.5\n",
      "Episode: 434, Reward: 156.0, Mean of 100 cons episodes: 110.91\n",
      "Episode: 435, Reward: 158.0, Mean of 100 cons episodes: 111.87\n",
      "Episode: 436, Reward: 77.0, Mean of 100 cons episodes: 112.58\n",
      "Episode: 437, Reward: 119.0, Mean of 100 cons episodes: 112.24\n",
      "Episode: 438, Reward: 132.0, Mean of 100 cons episodes: 112.25\n",
      "Episode: 439, Reward: 200.0, Mean of 100 cons episodes: 112.88\n",
      "Episode: 440, Reward: 140.0, Mean of 100 cons episodes: 113.09\n",
      "Episode: 441, Reward: 80.0, Mean of 100 cons episodes: 114.1\n",
      "Episode: 442, Reward: 200.0, Mean of 100 cons episodes: 114.27\n",
      "Episode: 443, Reward: 200.0, Mean of 100 cons episodes: 114.84\n",
      "Episode: 444, Reward: 200.0, Mean of 100 cons episodes: 116.42\n",
      "Episode: 445, Reward: 133.0, Mean of 100 cons episodes: 117.39\n",
      "Episode: 446, Reward: 141.0, Mean of 100 cons episodes: 118.3\n",
      "Episode: 447, Reward: 163.0, Mean of 100 cons episodes: 118.78\n",
      "Episode: 448, Reward: 140.0, Mean of 100 cons episodes: 120.02\n",
      "Episode: 449, Reward: 146.0, Mean of 100 cons episodes: 121.08\n",
      "Episode: 450, Reward: 200.0, Mean of 100 cons episodes: 121.88\n",
      "Episode: 451, Reward: 116.0, Mean of 100 cons episodes: 123.03\n",
      "Episode: 452, Reward: 54.0, Mean of 100 cons episodes: 124.05\n",
      "Episode: 453, Reward: 195.0, Mean of 100 cons episodes: 123.87\n",
      "Episode: 454, Reward: 163.0, Mean of 100 cons episodes: 123.24\n",
      "Episode: 455, Reward: 132.0, Mean of 100 cons episodes: 124.18\n",
      "Episode: 456, Reward: 128.0, Mean of 100 cons episodes: 123.81\n",
      "Episode: 457, Reward: 124.0, Mean of 100 cons episodes: 124.05\n",
      "Episode: 458, Reward: 200.0, Mean of 100 cons episodes: 124.42\n",
      "Episode: 459, Reward: 137.0, Mean of 100 cons episodes: 124.53\n",
      "Episode: 460, Reward: 200.0, Mean of 100 cons episodes: 126.23\n",
      "Episode: 461, Reward: 190.0, Mean of 100 cons episodes: 126.91\n",
      "Episode: 462, Reward: 58.0, Mean of 100 cons episodes: 127.92\n",
      "Episode: 463, Reward: 136.0, Mean of 100 cons episodes: 127.82\n",
      "Episode: 464, Reward: 51.0, Mean of 100 cons episodes: 127.48\n",
      "Episode: 465, Reward: 163.0, Mean of 100 cons episodes: 127.71\n",
      "Episode: 466, Reward: 151.0, Mean of 100 cons episodes: 127.01\n",
      "Episode: 467, Reward: 136.0, Mean of 100 cons episodes: 127.86\n",
      "Episode: 468, Reward: 200.0, Mean of 100 cons episodes: 128.74\n",
      "Episode: 469, Reward: 137.0, Mean of 100 cons episodes: 128.53\n",
      "Episode: 470, Reward: 200.0, Mean of 100 cons episodes: 129.67\n",
      "Episode: 471, Reward: 200.0, Mean of 100 cons episodes: 129.43\n",
      "Episode: 472, Reward: 55.0, Mean of 100 cons episodes: 130.47\n",
      "Episode: 473, Reward: 200.0, Mean of 100 cons episodes: 132.14\n",
      "Episode: 474, Reward: 116.0, Mean of 100 cons episodes: 131.52\n",
      "Episode: 475, Reward: 200.0, Mean of 100 cons episodes: 131.91\n",
      "Episode: 476, Reward: 145.0, Mean of 100 cons episodes: 131.85\n",
      "Episode: 477, Reward: 200.0, Mean of 100 cons episodes: 132.84\n",
      "Episode: 478, Reward: 169.0, Mean of 100 cons episodes: 133.7\n",
      "Episode: 479, Reward: 200.0, Mean of 100 cons episodes: 133.95\n",
      "Episode: 480, Reward: 113.0, Mean of 100 cons episodes: 134.68\n",
      "Episode: 481, Reward: 200.0, Mean of 100 cons episodes: 136.12\n",
      "Episode: 482, Reward: 72.0, Mean of 100 cons episodes: 136.25\n",
      "Episode: 483, Reward: 145.0, Mean of 100 cons episodes: 137.11\n",
      "Episode: 484, Reward: 200.0, Mean of 100 cons episodes: 136.6\n",
      "Episode: 485, Reward: 200.0, Mean of 100 cons episodes: 136.73\n",
      "Episode: 486, Reward: 167.0, Mean of 100 cons episodes: 137.52\n",
      "Episode: 487, Reward: 137.0, Mean of 100 cons episodes: 137.98\n",
      "Episode: 488, Reward: 200.0, Mean of 100 cons episodes: 138.61\n",
      "Episode: 489, Reward: 200.0, Mean of 100 cons episodes: 138.51\n",
      "Episode: 490, Reward: 200.0, Mean of 100 cons episodes: 138.51\n",
      "Episode: 491, Reward: 198.0, Mean of 100 cons episodes: 139.37\n",
      "Episode: 492, Reward: 121.0, Mean of 100 cons episodes: 140.31\n",
      "Episode: 493, Reward: 200.0, Mean of 100 cons episodes: 140.29\n",
      "Episode: 494, Reward: 200.0, Mean of 100 cons episodes: 140.8\n",
      "Episode: 495, Reward: 200.0, Mean of 100 cons episodes: 140.8\n",
      "Episode: 496, Reward: 157.0, Mean of 100 cons episodes: 141.94\n",
      "Episode: 497, Reward: 195.0, Mean of 100 cons episodes: 142.91\n",
      "Episode: 498, Reward: 163.0, Mean of 100 cons episodes: 143.59\n",
      "Episode: 499, Reward: 150.0, Mean of 100 cons episodes: 145.03\n",
      "Episode: 500, Reward: 200.0, Mean of 100 cons episodes: 145.18\n",
      "Episode: 501, Reward: 161.0, Mean of 100 cons episodes: 145.16\n",
      "Episode: 502, Reward: 200.0, Mean of 100 cons episodes: 145.98\n",
      "Episode: 503, Reward: 103.0, Mean of 100 cons episodes: 147.0\n",
      "Episode: 504, Reward: 200.0, Mean of 100 cons episodes: 148.15\n",
      "Episode: 505, Reward: 182.0, Mean of 100 cons episodes: 147.72\n",
      "Episode: 506, Reward: 196.0, Mean of 100 cons episodes: 148.09\n",
      "Episode: 507, Reward: 113.0, Mean of 100 cons episodes: 148.66\n",
      "Episode: 508, Reward: 200.0, Mean of 100 cons episodes: 149.38\n",
      "Episode: 509, Reward: 200.0, Mean of 100 cons episodes: 148.86\n",
      "Episode: 510, Reward: 182.0, Mean of 100 cons episodes: 149.92\n",
      "Episode: 511, Reward: 200.0, Mean of 100 cons episodes: 150.84\n",
      "Episode: 512, Reward: 200.0, Mean of 100 cons episodes: 151.82\n",
      "Episode: 513, Reward: 200.0, Mean of 100 cons episodes: 152.57\n",
      "Episode: 514, Reward: 103.0, Mean of 100 cons episodes: 152.57\n",
      "Episode: 515, Reward: 200.0, Mean of 100 cons episodes: 153.55\n",
      "Episode: 516, Reward: 133.0, Mean of 100 cons episodes: 154.03\n",
      "Episode: 517, Reward: 58.0, Mean of 100 cons episodes: 154.28\n",
      "Episode: 518, Reward: 131.0, Mean of 100 cons episodes: 154.76\n",
      "Episode: 519, Reward: 200.0, Mean of 100 cons episodes: 154.24\n",
      "Episode: 520, Reward: 126.0, Mean of 100 cons episodes: 154.54\n",
      "Episode: 521, Reward: 91.0, Mean of 100 cons episodes: 155.37\n",
      "Episode: 522, Reward: 135.0, Mean of 100 cons episodes: 155.77\n",
      "Episode: 523, Reward: 200.0, Mean of 100 cons episodes: 155.1\n",
      "Episode: 524, Reward: 174.0, Mean of 100 cons episodes: 155.21\n",
      "Episode: 525, Reward: 141.0, Mean of 100 cons episodes: 156.06\n",
      "Episode: 526, Reward: 200.0, Mean of 100 cons episodes: 157.14\n",
      "Episode: 527, Reward: 154.0, Mean of 100 cons episodes: 157.36\n",
      "Episode: 528, Reward: 200.0, Mean of 100 cons episodes: 158.14\n",
      "Episode: 529, Reward: 200.0, Mean of 100 cons episodes: 157.68\n",
      "Episode: 530, Reward: 200.0, Mean of 100 cons episodes: 158.62\n",
      "Episode: 531, Reward: 180.0, Mean of 100 cons episodes: 159.67\n",
      "Episode: 532, Reward: 119.0, Mean of 100 cons episodes: 159.99\n",
      "Episode: 533, Reward: 200.0, Mean of 100 cons episodes: 161.22\n",
      "Episode: 534, Reward: 200.0, Mean of 100 cons episodes: 160.41\n",
      "Episode: 535, Reward: 200.0, Mean of 100 cons episodes: 160.41\n",
      "Episode: 536, Reward: 157.0, Mean of 100 cons episodes: 160.85\n",
      "Episode: 537, Reward: 200.0, Mean of 100 cons episodes: 161.27\n",
      "Episode: 538, Reward: 122.0, Mean of 100 cons episodes: 162.07\n",
      "Episode: 539, Reward: 171.0, Mean of 100 cons episodes: 162.88\n",
      "Episode: 540, Reward: 93.0, Mean of 100 cons episodes: 162.78\n",
      "Episode: 541, Reward: 200.0, Mean of 100 cons episodes: 162.49\n",
      "Episode: 542, Reward: 200.0, Mean of 100 cons episodes: 162.02\n",
      "Episode: 543, Reward: 139.0, Mean of 100 cons episodes: 163.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 544, Reward: 200.0, Mean of 100 cons episodes: 163.22\n",
      "Episode: 545, Reward: 37.0, Mean of 100 cons episodes: 162.61\n",
      "Episode: 546, Reward: 161.0, Mean of 100 cons episodes: 162.61\n",
      "Episode: 547, Reward: 163.0, Mean of 100 cons episodes: 161.65\n",
      "Episode: 548, Reward: 200.0, Mean of 100 cons episodes: 161.85\n",
      "Episode: 549, Reward: 200.0, Mean of 100 cons episodes: 161.85\n",
      "Episode: 550, Reward: 165.0, Mean of 100 cons episodes: 162.45\n",
      "Episode: 551, Reward: 134.0, Mean of 100 cons episodes: 162.99\n",
      "Episode: 552, Reward: 200.0, Mean of 100 cons episodes: 162.64\n",
      "Episode: 553, Reward: 96.0, Mean of 100 cons episodes: 162.82\n",
      "Episode: 554, Reward: 200.0, Mean of 100 cons episodes: 164.28\n",
      "Episode: 555, Reward: 200.0, Mean of 100 cons episodes: 163.29\n",
      "Episode: 556, Reward: 200.0, Mean of 100 cons episodes: 163.66\n",
      "Episode: 557, Reward: 195.0, Mean of 100 cons episodes: 164.34\n",
      "Episode: 558, Reward: 200.0, Mean of 100 cons episodes: 165.06\n",
      "Episode: 559, Reward: 161.0, Mean of 100 cons episodes: 165.77\n",
      "Episode: 560, Reward: 200.0, Mean of 100 cons episodes: 165.77\n",
      "Episode: 561, Reward: 142.0, Mean of 100 cons episodes: 166.01\n",
      "Episode: 562, Reward: 165.0, Mean of 100 cons episodes: 166.01\n",
      "Episode: 563, Reward: 200.0, Mean of 100 cons episodes: 165.53\n",
      "Episode: 564, Reward: 126.0, Mean of 100 cons episodes: 166.6\n",
      "Episode: 565, Reward: 150.0, Mean of 100 cons episodes: 167.24\n",
      "Episode: 566, Reward: 200.0, Mean of 100 cons episodes: 167.99\n",
      "Episode: 567, Reward: 171.0, Mean of 100 cons episodes: 167.86\n",
      "Episode: 568, Reward: 133.0, Mean of 100 cons episodes: 168.35\n",
      "Episode: 569, Reward: 121.0, Mean of 100 cons episodes: 168.7\n",
      "Episode: 570, Reward: 185.0, Mean of 100 cons episodes: 168.03\n",
      "Episode: 571, Reward: 200.0, Mean of 100 cons episodes: 167.87\n",
      "Episode: 572, Reward: 200.0, Mean of 100 cons episodes: 167.72\n",
      "Episode: 573, Reward: 200.0, Mean of 100 cons episodes: 167.72\n",
      "Episode: 574, Reward: 164.0, Mean of 100 cons episodes: 169.17\n",
      "Episode: 575, Reward: 200.0, Mean of 100 cons episodes: 169.17\n",
      "Episode: 576, Reward: 200.0, Mean of 100 cons episodes: 169.65\n",
      "Episode: 577, Reward: 200.0, Mean of 100 cons episodes: 169.65\n",
      "Episode: 578, Reward: 118.0, Mean of 100 cons episodes: 170.2\n",
      "Episode: 579, Reward: 200.0, Mean of 100 cons episodes: 170.2\n",
      "Episode: 580, Reward: 111.0, Mean of 100 cons episodes: 169.69\n",
      "Episode: 581, Reward: 200.0, Mean of 100 cons episodes: 169.69\n",
      "Episode: 582, Reward: 113.0, Mean of 100 cons episodes: 169.67\n",
      "Episode: 583, Reward: 174.0, Mean of 100 cons episodes: 169.67\n",
      "Episode: 584, Reward: 131.0, Mean of 100 cons episodes: 170.08\n",
      "Episode: 585, Reward: 159.0, Mean of 100 cons episodes: 170.37\n",
      "Episode: 586, Reward: 200.0, Mean of 100 cons episodes: 169.68\n",
      "Episode: 587, Reward: 200.0, Mean of 100 cons episodes: 169.27\n",
      "Episode: 588, Reward: 178.0, Mean of 100 cons episodes: 169.6\n",
      "Episode: 589, Reward: 131.0, Mean of 100 cons episodes: 170.23\n",
      "Episode: 590, Reward: 200.0, Mean of 100 cons episodes: 170.01\n",
      "Episode: 591, Reward: 151.0, Mean of 100 cons episodes: 169.32\n",
      "Episode: 592, Reward: 118.0, Mean of 100 cons episodes: 169.32\n",
      "Episode: 593, Reward: 182.0, Mean of 100 cons episodes: 168.85\n",
      "Episode: 594, Reward: 152.0, Mean of 100 cons episodes: 168.82\n",
      "Episode: 595, Reward: 200.0, Mean of 100 cons episodes: 168.64\n",
      "Episode: 596, Reward: 122.0, Mean of 100 cons episodes: 168.16\n",
      "Episode: 597, Reward: 68.0, Mean of 100 cons episodes: 168.16\n",
      "Episode: 598, Reward: 127.0, Mean of 100 cons episodes: 167.81\n",
      "Episode: 599, Reward: 160.0, Mean of 100 cons episodes: 166.54\n",
      "Episode: 600, Reward: 189.0, Mean of 100 cons episodes: 166.18\n",
      "Episode: 601, Reward: 200.0, Mean of 100 cons episodes: 166.28\n",
      "Episode: 602, Reward: 167.0, Mean of 100 cons episodes: 166.17\n",
      "Episode: 603, Reward: 200.0, Mean of 100 cons episodes: 166.56\n",
      "Episode: 604, Reward: 192.0, Mean of 100 cons episodes: 166.23\n",
      "Episode: 605, Reward: 200.0, Mean of 100 cons episodes: 167.2\n",
      "Episode: 606, Reward: 200.0, Mean of 100 cons episodes: 167.12\n",
      "Episode: 607, Reward: 185.0, Mean of 100 cons episodes: 167.3\n",
      "Episode: 608, Reward: 200.0, Mean of 100 cons episodes: 167.34\n",
      "Episode: 609, Reward: 200.0, Mean of 100 cons episodes: 168.06\n",
      "Episode: 610, Reward: 195.0, Mean of 100 cons episodes: 168.06\n",
      "Episode: 611, Reward: 200.0, Mean of 100 cons episodes: 168.06\n",
      "Episode: 612, Reward: 180.0, Mean of 100 cons episodes: 168.19\n",
      "Episode: 613, Reward: 200.0, Mean of 100 cons episodes: 168.19\n",
      "Episode: 614, Reward: 200.0, Mean of 100 cons episodes: 167.99\n",
      "Episode: 615, Reward: 168.0, Mean of 100 cons episodes: 167.99\n",
      "Episode: 616, Reward: 200.0, Mean of 100 cons episodes: 168.96\n",
      "Episode: 617, Reward: 76.0, Mean of 100 cons episodes: 168.64\n",
      "Episode: 618, Reward: 200.0, Mean of 100 cons episodes: 169.31\n",
      "Episode: 619, Reward: 140.0, Mean of 100 cons episodes: 169.49\n",
      "Episode: 620, Reward: 200.0, Mean of 100 cons episodes: 170.18\n",
      "Episode: 621, Reward: 200.0, Mean of 100 cons episodes: 169.58\n",
      "Episode: 622, Reward: 200.0, Mean of 100 cons episodes: 170.32\n",
      "Episode: 623, Reward: 180.0, Mean of 100 cons episodes: 171.41\n",
      "Episode: 624, Reward: 200.0, Mean of 100 cons episodes: 172.06\n",
      "Episode: 625, Reward: 200.0, Mean of 100 cons episodes: 171.86\n",
      "Episode: 626, Reward: 164.0, Mean of 100 cons episodes: 172.12\n",
      "Episode: 627, Reward: 159.0, Mean of 100 cons episodes: 172.71\n",
      "Episode: 628, Reward: 200.0, Mean of 100 cons episodes: 172.35\n",
      "Episode: 629, Reward: 200.0, Mean of 100 cons episodes: 172.4\n",
      "Episode: 630, Reward: 200.0, Mean of 100 cons episodes: 172.4\n",
      "Episode: 631, Reward: 200.0, Mean of 100 cons episodes: 172.4\n",
      "Episode: 632, Reward: 200.0, Mean of 100 cons episodes: 172.4\n",
      "Episode: 633, Reward: 200.0, Mean of 100 cons episodes: 172.6\n",
      "Episode: 634, Reward: 140.0, Mean of 100 cons episodes: 173.41\n",
      "Episode: 635, Reward: 200.0, Mean of 100 cons episodes: 173.41\n",
      "Episode: 636, Reward: 200.0, Mean of 100 cons episodes: 172.81\n",
      "Episode: 637, Reward: 200.0, Mean of 100 cons episodes: 172.81\n",
      "Episode: 638, Reward: 200.0, Mean of 100 cons episodes: 173.24\n",
      "Episode: 639, Reward: 181.0, Mean of 100 cons episodes: 173.24\n",
      "Episode: 640, Reward: 200.0, Mean of 100 cons episodes: 174.02\n",
      "Episode: 641, Reward: 200.0, Mean of 100 cons episodes: 174.12\n",
      "Episode: 642, Reward: 200.0, Mean of 100 cons episodes: 175.19\n",
      "Episode: 643, Reward: 200.0, Mean of 100 cons episodes: 175.19\n",
      "Episode: 644, Reward: 200.0, Mean of 100 cons episodes: 175.19\n",
      "Episode: 645, Reward: 189.0, Mean of 100 cons episodes: 175.8\n",
      "Episode: 646, Reward: 167.0, Mean of 100 cons episodes: 175.8\n",
      "Episode: 647, Reward: 200.0, Mean of 100 cons episodes: 177.32\n",
      "Episode: 648, Reward: 200.0, Mean of 100 cons episodes: 177.38\n",
      "Episode: 649, Reward: 200.0, Mean of 100 cons episodes: 177.75\n",
      "Episode: 650, Reward: 200.0, Mean of 100 cons episodes: 177.75\n",
      "Episode: 651, Reward: 200.0, Mean of 100 cons episodes: 177.75\n",
      "Episode: 652, Reward: 200.0, Mean of 100 cons episodes: 178.1\n",
      "Episode: 653, Reward: 200.0, Mean of 100 cons episodes: 178.76\n",
      "Episode: 654, Reward: 178.0, Mean of 100 cons episodes: 178.76\n",
      "Episode: 655, Reward: 123.0, Mean of 100 cons episodes: 179.8\n",
      "Episode: 656, Reward: 200.0, Mean of 100 cons episodes: 179.58\n",
      "Episode: 657, Reward: 200.0, Mean of 100 cons episodes: 178.81\n",
      "Episode: 658, Reward: 200.0, Mean of 100 cons episodes: 178.81\n",
      "Episode: 659, Reward: 200.0, Mean of 100 cons episodes: 178.86\n",
      "Episode: 660, Reward: 200.0, Mean of 100 cons episodes: 178.86\n",
      "Episode: 661, Reward: 165.0, Mean of 100 cons episodes: 179.25\n",
      "Episode: 662, Reward: 167.0, Mean of 100 cons episodes: 179.25\n",
      "Episode: 663, Reward: 200.0, Mean of 100 cons episodes: 179.48\n",
      "Episode: 664, Reward: 119.0, Mean of 100 cons episodes: 179.5\n",
      "Episode: 665, Reward: 200.0, Mean of 100 cons episodes: 179.5\n",
      "Episode: 666, Reward: 200.0, Mean of 100 cons episodes: 179.43\n",
      "Episode: 667, Reward: 200.0, Mean of 100 cons episodes: 179.93\n",
      "Episode: 668, Reward: 190.0, Mean of 100 cons episodes: 179.93\n",
      "Episode: 669, Reward: 200.0, Mean of 100 cons episodes: 180.22\n",
      "Episode: 670, Reward: 118.0, Mean of 100 cons episodes: 180.79\n",
      "Episode: 671, Reward: 200.0, Mean of 100 cons episodes: 181.58\n",
      "Episode: 672, Reward: 183.0, Mean of 100 cons episodes: 180.91\n",
      "Episode: 673, Reward: 200.0, Mean of 100 cons episodes: 180.91\n",
      "Episode: 674, Reward: 161.0, Mean of 100 cons episodes: 180.74\n",
      "Episode: 675, Reward: 200.0, Mean of 100 cons episodes: 180.74\n",
      "Episode: 676, Reward: 200.0, Mean of 100 cons episodes: 180.71\n",
      "Episode: 677, Reward: 200.0, Mean of 100 cons episodes: 180.71\n",
      "Episode: 678, Reward: 200.0, Mean of 100 cons episodes: 180.71\n",
      "Episode: 679, Reward: 188.0, Mean of 100 cons episodes: 180.71\n",
      "Episode: 680, Reward: 200.0, Mean of 100 cons episodes: 181.53\n",
      "Episode: 681, Reward: 200.0, Mean of 100 cons episodes: 181.41\n",
      "Episode: 682, Reward: 116.0, Mean of 100 cons episodes: 182.3\n",
      "Episode: 683, Reward: 200.0, Mean of 100 cons episodes: 182.3\n",
      "Episode: 684, Reward: 197.0, Mean of 100 cons episodes: 182.33\n",
      "Episode: 685, Reward: 200.0, Mean of 100 cons episodes: 182.59\n",
      "Episode: 686, Reward: 200.0, Mean of 100 cons episodes: 183.25\n",
      "Episode: 687, Reward: 200.0, Mean of 100 cons episodes: 183.66\n",
      "Episode: 688, Reward: 200.0, Mean of 100 cons episodes: 183.66\n",
      "Episode: 689, Reward: 200.0, Mean of 100 cons episodes: 183.66\n",
      "Episode: 690, Reward: 200.0, Mean of 100 cons episodes: 183.88\n",
      "Episode: 691, Reward: 122.0, Mean of 100 cons episodes: 184.57\n",
      "Episode: 692, Reward: 200.0, Mean of 100 cons episodes: 184.57\n",
      "Episode: 693, Reward: 200.0, Mean of 100 cons episodes: 184.28\n",
      "Episode: 694, Reward: 200.0, Mean of 100 cons episodes: 185.1\n",
      "Episode: 695, Reward: 172.0, Mean of 100 cons episodes: 185.28\n",
      "Episode: 696, Reward: 200.0, Mean of 100 cons episodes: 185.76\n",
      "Episode: 697, Reward: 200.0, Mean of 100 cons episodes: 185.48\n",
      "Episode: 698, Reward: 200.0, Mean of 100 cons episodes: 186.26\n",
      "Episode: 699, Reward: 200.0, Mean of 100 cons episodes: 187.58\n",
      "Episode: 700, Reward: 200.0, Mean of 100 cons episodes: 188.31\n",
      "Episode: 701, Reward: 200.0, Mean of 100 cons episodes: 188.71\n",
      "Episode: 702, Reward: 200.0, Mean of 100 cons episodes: 188.82\n",
      "Episode: 703, Reward: 200.0, Mean of 100 cons episodes: 188.82\n",
      "Episode: 704, Reward: 173.0, Mean of 100 cons episodes: 189.15\n",
      "Episode: 705, Reward: 200.0, Mean of 100 cons episodes: 189.15\n",
      "Episode: 706, Reward: 200.0, Mean of 100 cons episodes: 188.96\n",
      "Episode: 707, Reward: 200.0, Mean of 100 cons episodes: 188.96\n",
      "Episode: 708, Reward: 200.0, Mean of 100 cons episodes: 188.96\n",
      "Episode: 709, Reward: 200.0, Mean of 100 cons episodes: 189.11\n",
      "Episode: 710, Reward: 200.0, Mean of 100 cons episodes: 189.11\n",
      "Episode: 711, Reward: 200.0, Mean of 100 cons episodes: 189.11\n",
      "Episode: 712, Reward: 200.0, Mean of 100 cons episodes: 189.16\n",
      "Episode: 713, Reward: 77.0, Mean of 100 cons episodes: 189.16\n",
      "Episode: 714, Reward: 183.0, Mean of 100 cons episodes: 189.36\n",
      "Episode: 715, Reward: 200.0, Mean of 100 cons episodes: 188.13\n",
      "Episode: 716, Reward: 200.0, Mean of 100 cons episodes: 187.96\n",
      "Episode: 717, Reward: 162.0, Mean of 100 cons episodes: 188.28\n",
      "Episode: 718, Reward: 200.0, Mean of 100 cons episodes: 188.28\n",
      "Episode: 719, Reward: 200.0, Mean of 100 cons episodes: 189.14\n",
      "Episode: 720, Reward: 197.0, Mean of 100 cons episodes: 189.14\n",
      "Episode: 721, Reward: 200.0, Mean of 100 cons episodes: 189.74\n",
      "Episode: 722, Reward: 200.0, Mean of 100 cons episodes: 189.71\n",
      "Episode: 723, Reward: 200.0, Mean of 100 cons episodes: 189.71\n",
      "Episode: 724, Reward: 200.0, Mean of 100 cons episodes: 189.71\n",
      "Episode: 725, Reward: 200.0, Mean of 100 cons episodes: 189.91\n",
      "Episode: 726, Reward: 200.0, Mean of 100 cons episodes: 189.91\n",
      "Episode: 727, Reward: 195.0, Mean of 100 cons episodes: 189.91\n",
      "Episode: 728, Reward: 181.0, Mean of 100 cons episodes: 190.27\n",
      "Episode: 729, Reward: 80.0, Mean of 100 cons episodes: 190.63\n",
      "Episode: 730, Reward: 200.0, Mean of 100 cons episodes: 190.44\n",
      "Episode: 731, Reward: 200.0, Mean of 100 cons episodes: 189.24\n",
      "Episode: 732, Reward: 200.0, Mean of 100 cons episodes: 189.24\n",
      "Episode: 733, Reward: 186.0, Mean of 100 cons episodes: 189.24\n",
      "Episode: 734, Reward: 126.0, Mean of 100 cons episodes: 189.24\n",
      "Episode: 735, Reward: 200.0, Mean of 100 cons episodes: 189.1\n",
      "Episode: 736, Reward: 200.0, Mean of 100 cons episodes: 188.96\n",
      "Episode: 737, Reward: 200.0, Mean of 100 cons episodes: 188.96\n",
      "Episode: 738, Reward: 200.0, Mean of 100 cons episodes: 188.96\n",
      "Episode: 739, Reward: 200.0, Mean of 100 cons episodes: 188.96\n",
      "Episode: 740, Reward: 140.0, Mean of 100 cons episodes: 188.96\n",
      "Episode: 741, Reward: 200.0, Mean of 100 cons episodes: 189.15\n",
      "Episode: 742, Reward: 200.0, Mean of 100 cons episodes: 188.55\n",
      "Episode: 743, Reward: 200.0, Mean of 100 cons episodes: 188.55\n",
      "Episode: 744, Reward: 200.0, Mean of 100 cons episodes: 188.55\n",
      "Episode: 745, Reward: 200.0, Mean of 100 cons episodes: 188.55\n",
      "Episode: 746, Reward: 200.0, Mean of 100 cons episodes: 188.55\n",
      "Episode: 747, Reward: 200.0, Mean of 100 cons episodes: 188.66\n",
      "Episode: 748, Reward: 200.0, Mean of 100 cons episodes: 188.99\n",
      "Episode: 749, Reward: 83.0, Mean of 100 cons episodes: 188.99\n",
      "Episode: 750, Reward: 200.0, Mean of 100 cons episodes: 188.99\n",
      "Episode: 751, Reward: 200.0, Mean of 100 cons episodes: 187.82\n",
      "Episode: 752, Reward: 200.0, Mean of 100 cons episodes: 187.82\n",
      "Episode: 753, Reward: 200.0, Mean of 100 cons episodes: 187.82\n",
      "Episode: 754, Reward: 200.0, Mean of 100 cons episodes: 187.82\n",
      "Episode: 755, Reward: 200.0, Mean of 100 cons episodes: 187.82\n",
      "Episode: 756, Reward: 182.0, Mean of 100 cons episodes: 188.04\n",
      "Episode: 757, Reward: 200.0, Mean of 100 cons episodes: 188.81\n",
      "Episode: 758, Reward: 200.0, Mean of 100 cons episodes: 188.63\n",
      "Episode: 759, Reward: 200.0, Mean of 100 cons episodes: 188.63\n",
      "Episode: 760, Reward: 200.0, Mean of 100 cons episodes: 188.63\n",
      "Episode: 761, Reward: 200.0, Mean of 100 cons episodes: 188.63\n",
      "Episode: 762, Reward: 200.0, Mean of 100 cons episodes: 188.63\n",
      "Episode: 763, Reward: 200.0, Mean of 100 cons episodes: 188.98\n",
      "Episode: 764, Reward: 200.0, Mean of 100 cons episodes: 189.31\n",
      "Episode: 765, Reward: 152.0, Mean of 100 cons episodes: 189.31\n",
      "Episode: 766, Reward: 200.0, Mean of 100 cons episodes: 190.12\n",
      "Episode: 767, Reward: 200.0, Mean of 100 cons episodes: 189.64\n",
      "Episode: 768, Reward: 200.0, Mean of 100 cons episodes: 189.64\n",
      "Episode: 769, Reward: 200.0, Mean of 100 cons episodes: 189.64\n",
      "Episode: 770, Reward: 200.0, Mean of 100 cons episodes: 189.74\n",
      "Episode: 771, Reward: 200.0, Mean of 100 cons episodes: 189.74\n",
      "Episode: 772, Reward: 200.0, Mean of 100 cons episodes: 190.56\n",
      "Episode: 773, Reward: 200.0, Mean of 100 cons episodes: 190.56\n",
      "Episode: 774, Reward: 200.0, Mean of 100 cons episodes: 190.73\n",
      "Episode: 775, Reward: 200.0, Mean of 100 cons episodes: 190.73\n",
      "Episode: 776, Reward: 200.0, Mean of 100 cons episodes: 191.12\n",
      "Episode: 777, Reward: 200.0, Mean of 100 cons episodes: 191.12\n",
      "Episode: 778, Reward: 200.0, Mean of 100 cons episodes: 191.12\n",
      "Episode: 779, Reward: 200.0, Mean of 100 cons episodes: 191.12\n",
      "Episode: 780, Reward: 200.0, Mean of 100 cons episodes: 191.12\n",
      "Episode: 781, Reward: 200.0, Mean of 100 cons episodes: 191.24\n",
      "Episode: 782, Reward: 200.0, Mean of 100 cons episodes: 191.24\n",
      "Episode: 783, Reward: 200.0, Mean of 100 cons episodes: 191.24\n",
      "Episode: 784, Reward: 200.0, Mean of 100 cons episodes: 192.08\n",
      "Episode: 785, Reward: 200.0, Mean of 100 cons episodes: 192.08\n",
      "Episode: 786, Reward: 200.0, Mean of 100 cons episodes: 192.11\n",
      "Episode: 787, Reward: 200.0, Mean of 100 cons episodes: 192.11\n",
      "Episode: 788, Reward: 200.0, Mean of 100 cons episodes: 192.11\n",
      "Episode: 789, Reward: 200.0, Mean of 100 cons episodes: 192.11\n",
      "Episode: 790, Reward: 144.0, Mean of 100 cons episodes: 192.11\n",
      "Episode: 791, Reward: 200.0, Mean of 100 cons episodes: 192.11\n",
      "Episode: 792, Reward: 200.0, Mean of 100 cons episodes: 191.55\n",
      "Episode: 793, Reward: 200.0, Mean of 100 cons episodes: 192.33\n",
      "Episode: 794, Reward: 200.0, Mean of 100 cons episodes: 192.33\n",
      "Episode: 795, Reward: 200.0, Mean of 100 cons episodes: 192.33\n",
      "Episode: 796, Reward: 200.0, Mean of 100 cons episodes: 192.33\n",
      "Episode: 797, Reward: 200.0, Mean of 100 cons episodes: 192.61\n",
      "Episode: 798, Reward: 200.0, Mean of 100 cons episodes: 192.61\n",
      "Episode: 799, Reward: 200.0, Mean of 100 cons episodes: 192.61\n",
      "Episode: 800, Reward: 200.0, Mean of 100 cons episodes: 192.61\n",
      "Episode: 801, Reward: 200.0, Mean of 100 cons episodes: 192.61\n",
      "Episode: 802, Reward: 200.0, Mean of 100 cons episodes: 192.61\n",
      "Episode: 803, Reward: 200.0, Mean of 100 cons episodes: 192.61\n",
      "Episode: 804, Reward: 200.0, Mean of 100 cons episodes: 192.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 805, Reward: 200.0, Mean of 100 cons episodes: 192.61\n",
      "Episode: 806, Reward: 200.0, Mean of 100 cons episodes: 192.88\n",
      "Episode: 807, Reward: 200.0, Mean of 100 cons episodes: 192.88\n",
      "Episode: 808, Reward: 200.0, Mean of 100 cons episodes: 192.88\n",
      "Episode: 809, Reward: 192.0, Mean of 100 cons episodes: 192.88\n",
      "Episode: 810, Reward: 200.0, Mean of 100 cons episodes: 192.88\n",
      "Episode: 811, Reward: 200.0, Mean of 100 cons episodes: 192.8\n",
      "Episode: 812, Reward: 200.0, Mean of 100 cons episodes: 192.8\n",
      "Episode: 813, Reward: 191.0, Mean of 100 cons episodes: 192.8\n",
      "Episode: 814, Reward: 200.0, Mean of 100 cons episodes: 192.8\n",
      "Episode: 815, Reward: 200.0, Mean of 100 cons episodes: 193.94\n",
      "Episode: 816, Reward: 200.0, Mean of 100 cons episodes: 194.11\n",
      "Episode: 817, Reward: 200.0, Mean of 100 cons episodes: 194.11\n",
      "Episode: 818, Reward: 200.0, Mean of 100 cons episodes: 194.11\n",
      "Episode: 819, Reward: 200.0, Mean of 100 cons episodes: 194.49\n",
      "Episode: 820, Reward: 200.0, Mean of 100 cons episodes: 194.49\n",
      "Episode: 821, Reward: 200.0, Mean of 100 cons episodes: 194.49\n",
      "Episode: 822, Reward: 200.0, Mean of 100 cons episodes: 194.52\n",
      "Episode: 823, Reward: 200.0, Mean of 100 cons episodes: 194.52\n",
      "Episode: 824, Reward: 200.0, Mean of 100 cons episodes: 194.52\n",
      "Episode: 825, Reward: 200.0, Mean of 100 cons episodes: 194.52\n",
      "Episode: 826, Reward: 200.0, Mean of 100 cons episodes: 194.52\n",
      "Episode: 827, Reward: 200.0, Mean of 100 cons episodes: 194.52\n",
      "Episode: 828, Reward: 200.0, Mean of 100 cons episodes: 194.52\n",
      "Episode: 829, Reward: 200.0, Mean of 100 cons episodes: 194.57\n",
      "Episode: 830, Reward: 200.0, Mean of 100 cons episodes: 194.76\n",
      "Episode: 831, Reward: 200.0, Mean of 100 cons episodes: 195.96\n",
      "Problem is solved.\n",
      "WARNING:tensorflow:From C:\\Users\\farad59\\Desktop\\Works\\vir_env\\gym\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\farad59\\Desktop\\Works\\vir_env\\gym\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: /tmp/cartpole_exp1/PG/agent_041120200946\\assets\n",
      "\n",
      "\n",
      "Problem is solved after 831 Episode with the mean reward 195.96 over the last 100 episodes\n"
     ]
    }
   ],
   "source": [
    "tot_rews = []\n",
    "mean_100ep = 0\n",
    "for episode in range(agent_par['num_episodes']):\n",
    "\n",
    "    # Do one rollout\n",
    "    states, actions, rewards, _, _ = CP.one_rollout(policy)\n",
    "\n",
    "    # Update the network\n",
    "    loss = policy.update_network(states, actions, rewards)\n",
    "\n",
    "    # Check if the problem is solved\n",
    "    if episode > 100:\n",
    "        mean_100ep = np.mean(tot_rews[-101:-1])\n",
    "\n",
    "    tot_reward = sum(rewards)\n",
    "    tot_rews.append(tot_reward)\n",
    "    print(f\"Episode: {episode}, Reward: {tot_reward}, Mean of 100 cons episodes: {mean_100ep}\")\n",
    "    if mean_100ep > env_par['threshold']:\n",
    "        print(f\"Problem is solved.\")\n",
    "        policy.network.save(agent_path)\n",
    "        break\n",
    "\n",
    "    # Save data\n",
    "    with train_writer.as_default():\n",
    "        tf.summary.scalar('reward', tot_reward, step=episode)\n",
    "\n",
    "# Close the environment\n",
    "CP.env.close()\n",
    "\n",
    "# Print the summary of the solution\n",
    "if mean_100ep > env_par['threshold']:\n",
    "    print(f\"\\n\\nProblem is solved after {episode} Episode with the mean reward {mean_100ep} over the last 100 episodes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "It will get around 1-2 minutes to run the above cell. You will probably get some WARNING\\ERROR. Some of these are related to incompatibility between some libraries. Don't panic. If you get the following at the end, the problem is solved successfully.\n",
    "\n",
    "<code>Problem is solved after 831 Episode with the mean reward 195.96 over the last 100 episodes<code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
