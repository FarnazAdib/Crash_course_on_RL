{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Control problems\n",
    "\n",
    "This is a self-contained repository to explain two basic Reinforcement (RL) algorithms, namely __Policy Gradient (PG)__ and __Q-learning__ from a control perspective. Dynamical systems might have discrete action-space like cartpole where two possible actions are +1 and -1 or continuous action space like linear gaussian systems. Usually, you can find a code for only one of these cases. It might be not obivous how to extend one to another. \n",
    "\n",
    "In this repository, I will explain how to formulate PG and Q-learning fo each of these cases. I will provide implementations for these algorithms for both cases as Jupyter notebooks. You can also find the pure code for these algorithm (and also a few more algorithms that I have implemented by not discussed). The code is easy to follow and read. I have written in a modular way, so for example if one is interested in the implentation of an algorithm is not confused with defining an environment in gym or plotting the results or so on.  \n",
    "\n",
    "Before starting running (and playing) with these algorithms, make sure you have built a virtual environment and imported required libraries. Take a look at [Preparation notebook](Preparation.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamical systems\n",
    "We consider two types of dynamical systems (or environment in RL terminology). Read about them here\n",
    "\n",
    "* [Cartpole: an environment with discrete action-space](cartpole.ipynb)\n",
    "* [Linear Gaussian: an environment with continuous action space](linear_quadratic.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "Below, you can see __jupyter notebooks__ regarding Policy Gradient Algorithm\n",
    "\n",
    "* [Policy Gradient (PG) explanation](pg_notebook.ipynb)\n",
    "    * [Discrete action space (cartpole)](pg_on_cartpole_notebook.ipynb)\n",
    "    * [Continuous action space (linear quadratic)](pg_on_lq_notebook.ipynb)\n",
    "    \n",
    "You can also see the __pure code__ for PG\n",
    "* PG pure code\n",
    "    * [PG for discrete action space (cartpole)](./cartpole/pg_on_cartpole.py)\n",
    "    * [PG for continuous action space (linear quadratic)](./lq/pg_on_lq.py)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $Q$-learning \n",
    "\n",
    "Below, you can see __jupyter notebooks__ regarding $Q$-learning Algorithm\n",
    "* [$Q$-learning explanation](q_notebook.ipynb)\n",
    "    * [Discrete action space (cartpole)](q_on_cartpole_notebook.ipynb)\n",
    "    * [Continuous action space (linear quadratic)](q_on_lq_notebook.ipynb)\n",
    "* [Experience replay $Q$-learning explanation](replay_q_notebook.ipynb)\n",
    "    * [Experience replay $Q$ learning for discrete action space (cartple)](replay_q_on_cartpole_notebook.ipynb)\n",
    "    * _We have not implemented explerience replay $Q$ learning on LQ problem because the plain $Q$-learning is super good on LQ porblem. Note that as you can see from the explanation in the experience replay $Q$-learning, this algorithm has only two simple functions in addition to the plain $Q$-learning and those are not related to the action to be discrete or continuous. So, I think this can be omitted here._\n",
    "    \n",
    "You can also see the __pure code__ for $Q$- and experience replay $Q$-learning\n",
    "    \n",
    "* $Q$-learning pure code\n",
    "    * [$Q$-learning for discrete action space (cartpole)](./cartpole/q_on_cartpole.py)\n",
    "    * [Experience replay $Q$ learning for discrete action space (cartple)](./cartpole/replay_q_on_cartpole.py)\n",
    "* Experience replay $Q$-learning pure code\n",
    "    * [$Q$-learning for continuous action space (linear quadratic)](./lq/q_on_lq.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
