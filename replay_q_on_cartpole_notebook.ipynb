{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFXo3MW1MiTM"
   },
   "source": [
    "# Experience replay $Q$-learning on cartpole\n",
    "Below, we run the experience replay $Q$-learning algorithm on the cartpole example. \n",
    "\n",
    "* [You can read about the cartpole problem here.](cartpole.ipynb)\n",
    "* [You can read about experience replay $Q$-learning here.](replay_q_notebook.ipynb)\n",
    "* [You can see the pure code for experience replay $Q$-learning on cartpole here.](./cartpole/replay_q_on_cartpole.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wLRP3dyMiTP"
   },
   "source": [
    "## Summary of the algorithm\n",
    "We build a (deep) network to represent $Q(s,a)$= `network(state)` and initiate an empty `memory=[]`\n",
    "\n",
    "```\n",
    "network = keras.Sequential([\n",
    "        keras.layers.Dense(30, input_dim=n_s, activation='relu'),\n",
    "        keras.layers.Dense(30, activation='relu'),\n",
    "        keras.layers.Dense(30, activation='relu'),\n",
    "        keras.layers.Dense(n_a)])\n",
    "        \n",
    "self.network.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqzmzzUAMiTR"
   },
   "source": [
    "Then, we iteratively improve the network. In each iteration of the algorithm, we do the following\n",
    "* i. We rollout the environment to collect data for experience replay $Q$-learning by following these steps:\n",
    "    * i.a. We observe the `state` $s$ and select the `action` $a$ according to\n",
    "    \n",
    "    ```\n",
    "    if np.random.random() <= epsilon:\n",
    "        selected_action = env.action_space.sample()\n",
    "\n",
    "    else:\n",
    "        selected_action = np.argmax(self.network(state))\n",
    "    ```\n",
    "    \n",
    " \n",
    "    * i.b. We derive the environment using $a$ and observe the `reward` $r$, the next state $s^{\\prime}$, and the boolean $done$ (which is `True` if the episode has ended and `False` otherwise).\n",
    "    * i.c. We add $s,\\:a,\\:r,\\:s^{\\prime},\\:done$ to `memory`.\n",
    "    * i.d. We continue from i.a. until the episode ends.\n",
    "* ii. We improve the $Q$ network\n",
    "    * ii.a. We sample a batch from `memory`. Let `states`, `actions`, `rewards`, `next_states`, `dones` denote the sampled batch.\n",
    "    \n",
    "    `batch = random.sample(self.memory, min(len(self.memory), batch_size))`\n",
    "    \n",
    "    `states, actions, rewards, new_states, dones = list(map(lambda i: [j[i] for j in batch], range(5)))`\n",
    "    \n",
    "    * ii.b. We supply `states`, `actions`, `rewards`, `next_states`, `dones` to the network and optimize the parameters of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJV0LfbQMiTT"
   },
   "source": [
    "## Running on google colab\n",
    "If you want to run on google colab, go ahead and run the following cell. If you want to run on your computer, skip this cell and start from Importing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5901,
     "status": "ok",
     "timestamp": 1604643358257,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "QIz2m9EJMiTU",
    "outputId": "2d51438e-90e1-4234-f3be-c77545a376d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Crash_course_on_RL'...\n",
      "remote: Enumerating objects: 164, done.\u001b[K\n",
      "remote: Counting objects: 100% (164/164), done.\u001b[K\n",
      "remote: Compressing objects: 100% (87/87), done.\u001b[K\n",
      "remote: Total 164 (delta 92), reused 147 (delta 75), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (164/164), 367.57 KiB | 17.50 MiB/s, done.\n",
      "Resolving deltas: 100% (92/92), done.\n",
      "/content/Crash_course_on_RL\n",
      "Processing /content/Crash_course_on_RL\n",
      "Building wheels for collected packages: Reinforcement-Learning-for-Control\n",
      "  Building wheel for Reinforcement-Learning-for-Control (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for Reinforcement-Learning-for-Control: filename=Reinforcement_Learning_for_Control-0.0.1-cp36-none-any.whl size=3832 sha256=2c9aa88606496da577a4dbae2777a4cae4597c52b88bca526285d53a1616365c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ub6tgf96/wheels/9b/1d/4f/f6799e86f243362b1f3fb259778269457c7c763ccf94aab885\n",
      "Successfully built Reinforcement-Learning-for-Control\n",
      "Installing collected packages: Reinforcement-Learning-for-Control\n",
      "Successfully installed Reinforcement-Learning-for-Control-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/FarnazAdib/Crash_course_on_RL.git\n",
    "%cd Crash_course_on_RL\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7SZ8T2CMiTf"
   },
   "source": [
    "## Importing libraries\n",
    "We start coding by importing the required libraries. If you get an error, you have possibly forgotten to change the kernel. See [Prepare a virtual environment](Preparation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2677,
     "status": "ok",
     "timestamp": 1604643364983,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "C-_uNEn9MiTh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "from collections import deque\n",
    "import datetime as dt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from cartpole.dynamics import CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7Y1NCXyMiTq"
   },
   "source": [
    "## Saving directories\n",
    "Next, we set up some paths to write data and capture some videos for future investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 694,
     "status": "ok",
     "timestamp": 1604643367967,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "bqa5LT5TMiTr"
   },
   "outputs": [],
   "source": [
    "STORE_PATH = '/tmp/cartpole_exp1/Q_replay'\n",
    "data_path = STORE_PATH + f\"/data_{dt.datetime.now().strftime('%d%m%Y%H%M')}\"\n",
    "agent_path = STORE_PATH + f\"/agent_{dt.datetime.now().strftime('%d%m%Y%H%M')}\"\n",
    "train_writer = tf.summary.create_file_writer(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqBex6snMiTw"
   },
   "source": [
    "## Making the environment\n",
    "We select the random seed and make the cartpole environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 637,
     "status": "ok",
     "timestamp": 1604643369706,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "LhrCuHCdMiTx"
   },
   "outputs": [],
   "source": [
    "Rand_Seed = 1\n",
    "env_par = {\n",
    "    'Rand_Seed': Rand_Seed,\n",
    "    'STORE_PATH': STORE_PATH,\n",
    "    'monitor': False,\n",
    "    'threshold': 195.0\n",
    "}\n",
    "Rand_Seed = 1\n",
    "CP = CartPole(env_par)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99rTjF6TMiT0"
   },
   "source": [
    "## Making the experience replay $Q$-learning agent\n",
    "We define the $Q$ learning class. This class receives a dictionary with the following entries:\n",
    "* `hidden_size`: Number of nodes in the layers.\n",
    "* `GAMMA`: forgetting factor in the total cost. It should be in $[0\\:1]$.\n",
    "* `num_episodes`: The maximum number of episodes to run.\n",
    "* `batch_size`: The memory length\n",
    "* `epsilon`: The probability of exploration. It should be in $(0\\:1)$.\n",
    "* `epsilon_decay`: The decay rate of epsilon.\n",
    "* `epsilon_min`: The minimum value of epsilon.\n",
    "* `learning_rate_adam`: The learning rate for adam optimization.\n",
    "* `adam_eps`: The epsilon in adam optimization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 727,
     "status": "ok",
     "timestamp": 1604643372921,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "it-nBFDHMiT1"
   },
   "outputs": [],
   "source": [
    "class Q:\n",
    "    def __init__(self, hparams, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.hparams = hparams\n",
    "        np.random.seed(hparams['Rand_Seed'])\n",
    "        tf.random.set_seed(hparams['Rand_Seed'])\n",
    "        random.seed(hparams['Rand_Seed'])\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.epsilon = hparams['epsilon']\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # The Q network\n",
    "        self.network = keras.Sequential([\n",
    "            keras.layers.Dense(self.hparams['hidden_size'], input_dim=self.hparams['num_state'], activation='relu',\n",
    "                               kernel_initializer=keras.initializers.he_normal(), dtype='float64'),\n",
    "            keras.layers.Dense(self.hparams['hidden_size'], activation='relu',\n",
    "                               kernel_initializer=keras.initializers.he_normal(), dtype='float64'),\n",
    "            keras.layers.Dense(self.hparams['hidden_size'], activation='relu',\n",
    "                               kernel_initializer=keras.initializers.he_normal(), dtype='float64'),\n",
    "            keras.layers.Dense(self.hparams['num_actions'], dtype='float64')\n",
    "        ])\n",
    "\n",
    "        # The cost function for the Q network\n",
    "        self.network.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(\n",
    "            epsilon=self.hparams['adam_eps'], learning_rate=self.hparams['learning_rate_adam']))\n",
    "\n",
    "    def get_action(self, state, env):\n",
    "        state = self._process_state(state)\n",
    "\n",
    "        # Exploration\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            selected_action = env.action_space.sample()\n",
    "\n",
    "        # Exploitation\n",
    "        else:\n",
    "            selected_action = np.argmax(self.network(state))\n",
    "        return selected_action\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # Adding data to the history batch\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # Sampling the history batch\n",
    "        batch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "        states, actions, rewards, new_states, dones = list(map(lambda i: [j[i] for j in batch], range(5)))\n",
    "        loss = self.update_network(states, actions, rewards, new_states, dones)\n",
    "        \n",
    "        # decreasing the exploration rate\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return loss\n",
    "\n",
    "    def update_network(self, states, actions, rewards, next_states, dones):\n",
    "        eps_length = len(states)\n",
    "        states = np.vstack(states)\n",
    "        q_target = self.network(states).numpy()\n",
    "        for i in range(eps_length):\n",
    "            if dones[i]:\n",
    "                q_target[i, actions[i]] = rewards[i]\n",
    "            else:\n",
    "                next_state = self._process_state(next_states[i])\n",
    "                q_target[i, actions[i]] = rewards[i] + \\\n",
    "                                        self.hparams['GAMMA'] * tf.math.reduce_max(self.network(next_state)).numpy()\n",
    "        loss = self.network.train_on_batch(states, q_target)\n",
    "        return loss\n",
    "\n",
    "    def _process_state(self, state):\n",
    "        return state.reshape([1, self.hparams['num_state']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M91w1FmGMiT5"
   },
   "source": [
    "The function `remember` adds the data point `state`, `action`, `reward`, `next_state`, `done` to the memeory history. In the experience replay, the history is sampled and the parameters of the network are updated using the sampled data. This is done by the function `replay(self, batch_size)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B15soUTPMiT6"
   },
   "source": [
    "Now that we have defined the experience replay $Q$-learning algorithm, it is enough to build an object and to iterate. You can change the following hyper parameters if you like\n",
    "\n",
    "* `hidden_size`: Number of nodes in the layers.\n",
    "* `GAMMA`: forgetting factor in the total cost. It should be in $[0\\:1]$.\n",
    "* `num_episodes`: The maximum number of episodes to run.\n",
    "* `batch_size`: The memory length\n",
    "* `epsilon`: The probability of exploration. It should be in $(0\\:1)$.\n",
    "* `epsilon_decay`: The decay rate of epsilon.\n",
    "* `epsilon_min`: The minimum value of epsilon.\n",
    "* `learning_rate_adam`: The learning rate for adam optimization.\n",
    "* `adam_eps`: The epsilon in adam optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 657,
     "status": "ok",
     "timestamp": 1604643376274,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "72S74oWNMiT8"
   },
   "outputs": [],
   "source": [
    "agent_par = {\n",
    "    'num_state': CP.env.observation_space.shape[0],\n",
    "    'num_actions': CP.env.action_space.n,\n",
    "    'Rand_Seed': Rand_Seed,\n",
    "    'hidden_size': 30,\n",
    "    'GAMMA': 1.0,\n",
    "    'num_episodes': 5000,\n",
    "    'batch_size': 200,\n",
    "    'epsilon': 0.1,  # exploration rate\n",
    "    'epsilon_decay': 0.995,\n",
    "    'epsilon_min': 0.01,\n",
    "    'learning_rate_adam': 0.01,\n",
    "    'adam_eps': 0.1\n",
    "}\n",
    "policy = Q(agent_par)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKlLWrtpMiUB"
   },
   "source": [
    "## Start learning\n",
    "Now, we start the learning loop. The learning loop iterates for a maximum of number of `num_episodes`. In each iteration\n",
    "* The agent derives the environment for one episode to collect data for experience replay $Q$-learning. The parameter `remember` is `Ture`. So, the data is saved in the `agent.memory`\n",
    "* We update the $Q$-network using experience replay.\n",
    "* We check if the problem is solved.\n",
    "* We write the data.\n",
    "At the end of the learning loop, we close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105852,
     "status": "ok",
     "timestamp": 1604643484186,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "cxlLCOVjMiUC",
    "outputId": "f52763ca-33a4-4dfe-cbc6-8a1222c93822"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Reward: 11.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 1, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 2, Reward: 11.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 3, Reward: 8.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 4, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 5, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 6, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 7, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 8, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 9, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 10, Reward: 13.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 11, Reward: 8.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 12, Reward: 12.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 13, Reward: 11.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 14, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 15, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 16, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 17, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 18, Reward: 11.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 19, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 20, Reward: 11.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 21, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 22, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 23, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 24, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 25, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 26, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 27, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 28, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 29, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 30, Reward: 11.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 31, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 32, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 33, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 34, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 35, Reward: 11.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 36, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 37, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 38, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 39, Reward: 11.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 40, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 41, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 42, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 43, Reward: 11.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 44, Reward: 12.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 45, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 46, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 47, Reward: 8.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 48, Reward: 12.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 49, Reward: 8.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 50, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 51, Reward: 8.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 52, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 53, Reward: 12.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 54, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 55, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 56, Reward: 11.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 57, Reward: 12.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 58, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 59, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 60, Reward: 12.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 61, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 62, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 63, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 64, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 65, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 66, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 67, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 68, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 69, Reward: 8.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 70, Reward: 11.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 71, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 72, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 73, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 74, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 75, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 76, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 77, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 78, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 79, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 80, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 81, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 82, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 83, Reward: 11.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 84, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 85, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 86, Reward: 11.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 87, Reward: 11.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 88, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 89, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 90, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 91, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 92, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 93, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 94, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 95, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 96, Reward: 13.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 97, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 98, Reward: 9.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 99, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 100, Reward: 10.0, Mean of 100 cons episodes: 0.0\n",
      "Episode: 101, Reward: 12.0, Mean of 100 cons episodes: 9.93\n",
      "Episode: 102, Reward: 11.0, Mean of 100 cons episodes: 9.92\n",
      "Episode: 103, Reward: 11.0, Mean of 100 cons episodes: 9.94\n",
      "Episode: 104, Reward: 12.0, Mean of 100 cons episodes: 9.94\n",
      "Episode: 105, Reward: 11.0, Mean of 100 cons episodes: 9.97\n",
      "Episode: 106, Reward: 14.0, Mean of 100 cons episodes: 9.99\n",
      "Episode: 107, Reward: 13.0, Mean of 100 cons episodes: 10.01\n",
      "Episode: 108, Reward: 14.0, Mean of 100 cons episodes: 10.06\n",
      "Episode: 109, Reward: 10.0, Mean of 100 cons episodes: 10.09\n",
      "Episode: 110, Reward: 12.0, Mean of 100 cons episodes: 10.13\n",
      "Episode: 111, Reward: 14.0, Mean of 100 cons episodes: 10.14\n",
      "Episode: 112, Reward: 11.0, Mean of 100 cons episodes: 10.13\n",
      "Episode: 113, Reward: 10.0, Mean of 100 cons episodes: 10.19\n",
      "Episode: 114, Reward: 11.0, Mean of 100 cons episodes: 10.18\n",
      "Episode: 115, Reward: 10.0, Mean of 100 cons episodes: 10.17\n",
      "Episode: 116, Reward: 16.0, Mean of 100 cons episodes: 10.19\n",
      "Episode: 117, Reward: 21.0, Mean of 100 cons episodes: 10.19\n",
      "Episode: 118, Reward: 17.0, Mean of 100 cons episodes: 10.26\n",
      "Episode: 119, Reward: 17.0, Mean of 100 cons episodes: 10.37\n",
      "Episode: 120, Reward: 33.0, Mean of 100 cons episodes: 10.43\n",
      "Episode: 121, Reward: 37.0, Mean of 100 cons episodes: 10.51\n",
      "Episode: 122, Reward: 36.0, Mean of 100 cons episodes: 10.73\n",
      "Episode: 123, Reward: 29.0, Mean of 100 cons episodes: 11.0\n",
      "Episode: 124, Reward: 47.0, Mean of 100 cons episodes: 11.26\n",
      "Episode: 125, Reward: 144.0, Mean of 100 cons episodes: 11.46\n",
      "Episode: 126, Reward: 146.0, Mean of 100 cons episodes: 11.83\n",
      "Episode: 127, Reward: 26.0, Mean of 100 cons episodes: 13.18\n",
      "Episode: 128, Reward: 29.0, Mean of 100 cons episodes: 14.55\n",
      "Episode: 129, Reward: 26.0, Mean of 100 cons episodes: 14.71\n",
      "Episode: 130, Reward: 28.0, Mean of 100 cons episodes: 14.9\n",
      "Episode: 131, Reward: 34.0, Mean of 100 cons episodes: 15.07\n",
      "Episode: 132, Reward: 85.0, Mean of 100 cons episodes: 15.24\n",
      "Episode: 133, Reward: 28.0, Mean of 100 cons episodes: 15.48\n",
      "Episode: 134, Reward: 15.0, Mean of 100 cons episodes: 16.23\n",
      "Episode: 135, Reward: 10.0, Mean of 100 cons episodes: 16.41\n",
      "Episode: 136, Reward: 9.0, Mean of 100 cons episodes: 16.46\n",
      "Episode: 137, Reward: 10.0, Mean of 100 cons episodes: 16.45\n",
      "Episode: 138, Reward: 10.0, Mean of 100 cons episodes: 16.45\n",
      "Episode: 139, Reward: 9.0, Mean of 100 cons episodes: 16.46\n",
      "Episode: 140, Reward: 9.0, Mean of 100 cons episodes: 16.46\n",
      "Episode: 141, Reward: 10.0, Mean of 100 cons episodes: 16.44\n",
      "Episode: 142, Reward: 10.0, Mean of 100 cons episodes: 16.43\n",
      "Episode: 143, Reward: 10.0, Mean of 100 cons episodes: 16.44\n",
      "Episode: 144, Reward: 21.0, Mean of 100 cons episodes: 16.44\n",
      "Episode: 145, Reward: 11.0, Mean of 100 cons episodes: 16.43\n",
      "Episode: 146, Reward: 9.0, Mean of 100 cons episodes: 16.52\n",
      "Episode: 147, Reward: 10.0, Mean of 100 cons episodes: 16.53\n",
      "Episode: 148, Reward: 9.0, Mean of 100 cons episodes: 16.52\n",
      "Episode: 149, Reward: 11.0, Mean of 100 cons episodes: 16.54\n",
      "Episode: 150, Reward: 10.0, Mean of 100 cons episodes: 16.51\n",
      "Episode: 151, Reward: 9.0, Mean of 100 cons episodes: 16.54\n",
      "Episode: 152, Reward: 10.0, Mean of 100 cons episodes: 16.55\n",
      "Episode: 153, Reward: 9.0, Mean of 100 cons episodes: 16.56\n",
      "Episode: 154, Reward: 11.0, Mean of 100 cons episodes: 16.56\n",
      "Episode: 155, Reward: 9.0, Mean of 100 cons episodes: 16.53\n",
      "Episode: 156, Reward: 9.0, Mean of 100 cons episodes: 16.54\n",
      "Episode: 157, Reward: 9.0, Mean of 100 cons episodes: 16.54\n",
      "Episode: 158, Reward: 9.0, Mean of 100 cons episodes: 16.52\n",
      "Episode: 159, Reward: 11.0, Mean of 100 cons episodes: 16.49\n",
      "Episode: 160, Reward: 9.0, Mean of 100 cons episodes: 16.49\n",
      "Episode: 161, Reward: 9.0, Mean of 100 cons episodes: 16.5\n",
      "Episode: 162, Reward: 11.0, Mean of 100 cons episodes: 16.47\n",
      "Episode: 163, Reward: 9.0, Mean of 100 cons episodes: 16.47\n",
      "Episode: 164, Reward: 10.0, Mean of 100 cons episodes: 16.48\n",
      "Episode: 165, Reward: 24.0, Mean of 100 cons episodes: 16.47\n",
      "Episode: 166, Reward: 10.0, Mean of 100 cons episodes: 16.47\n",
      "Episode: 167, Reward: 11.0, Mean of 100 cons episodes: 16.62\n",
      "Episode: 168, Reward: 10.0, Mean of 100 cons episodes: 16.63\n",
      "Episode: 169, Reward: 9.0, Mean of 100 cons episodes: 16.64\n",
      "Episode: 170, Reward: 11.0, Mean of 100 cons episodes: 16.64\n",
      "Episode: 171, Reward: 10.0, Mean of 100 cons episodes: 16.65\n",
      "Episode: 172, Reward: 16.0, Mean of 100 cons episodes: 16.65\n",
      "Episode: 173, Reward: 21.0, Mean of 100 cons episodes: 16.65\n",
      "Episode: 174, Reward: 200.0, Mean of 100 cons episodes: 16.72\n",
      "Episode: 175, Reward: 17.0, Mean of 100 cons episodes: 16.83\n",
      "Episode: 176, Reward: 13.0, Mean of 100 cons episodes: 18.74\n",
      "Episode: 177, Reward: 9.0, Mean of 100 cons episodes: 18.81\n",
      "Episode: 178, Reward: 10.0, Mean of 100 cons episodes: 18.84\n",
      "Episode: 179, Reward: 8.0, Mean of 100 cons episodes: 18.83\n",
      "Episode: 180, Reward: 11.0, Mean of 100 cons episodes: 18.84\n",
      "Episode: 181, Reward: 10.0, Mean of 100 cons episodes: 18.82\n",
      "Episode: 182, Reward: 10.0, Mean of 100 cons episodes: 18.83\n",
      "Episode: 183, Reward: 9.0, Mean of 100 cons episodes: 18.84\n",
      "Episode: 184, Reward: 8.0, Mean of 100 cons episodes: 18.84\n",
      "Episode: 185, Reward: 9.0, Mean of 100 cons episodes: 18.82\n",
      "Episode: 186, Reward: 9.0, Mean of 100 cons episodes: 18.8\n",
      "Episode: 187, Reward: 10.0, Mean of 100 cons episodes: 18.79\n",
      "Episode: 188, Reward: 10.0, Mean of 100 cons episodes: 18.77\n",
      "Episode: 189, Reward: 10.0, Mean of 100 cons episodes: 18.76\n",
      "Episode: 190, Reward: 10.0, Mean of 100 cons episodes: 18.76\n",
      "Episode: 191, Reward: 11.0, Mean of 100 cons episodes: 18.76\n",
      "Episode: 192, Reward: 10.0, Mean of 100 cons episodes: 18.76\n",
      "Episode: 193, Reward: 8.0, Mean of 100 cons episodes: 18.77\n",
      "Episode: 194, Reward: 9.0, Mean of 100 cons episodes: 18.78\n",
      "Episode: 195, Reward: 9.0, Mean of 100 cons episodes: 18.76\n",
      "Episode: 196, Reward: 9.0, Mean of 100 cons episodes: 18.75\n",
      "Episode: 197, Reward: 11.0, Mean of 100 cons episodes: 18.75\n",
      "Episode: 198, Reward: 11.0, Mean of 100 cons episodes: 18.71\n",
      "Episode: 199, Reward: 11.0, Mean of 100 cons episodes: 18.73\n",
      "Episode: 200, Reward: 12.0, Mean of 100 cons episodes: 18.75\n",
      "Episode: 201, Reward: 11.0, Mean of 100 cons episodes: 18.76\n",
      "Episode: 202, Reward: 12.0, Mean of 100 cons episodes: 18.78\n",
      "Episode: 203, Reward: 11.0, Mean of 100 cons episodes: 18.77\n",
      "Episode: 204, Reward: 13.0, Mean of 100 cons episodes: 18.78\n",
      "Episode: 205, Reward: 13.0, Mean of 100 cons episodes: 18.78\n",
      "Episode: 206, Reward: 14.0, Mean of 100 cons episodes: 18.79\n",
      "Episode: 207, Reward: 12.0, Mean of 100 cons episodes: 18.81\n",
      "Episode: 208, Reward: 15.0, Mean of 100 cons episodes: 18.81\n",
      "Episode: 209, Reward: 12.0, Mean of 100 cons episodes: 18.8\n",
      "Episode: 210, Reward: 16.0, Mean of 100 cons episodes: 18.81\n",
      "Episode: 211, Reward: 18.0, Mean of 100 cons episodes: 18.83\n",
      "Episode: 212, Reward: 15.0, Mean of 100 cons episodes: 18.87\n",
      "Episode: 213, Reward: 14.0, Mean of 100 cons episodes: 18.91\n",
      "Episode: 214, Reward: 17.0, Mean of 100 cons episodes: 18.95\n",
      "Episode: 215, Reward: 21.0, Mean of 100 cons episodes: 18.99\n",
      "Episode: 216, Reward: 15.0, Mean of 100 cons episodes: 19.05\n",
      "Episode: 217, Reward: 22.0, Mean of 100 cons episodes: 19.16\n",
      "Episode: 218, Reward: 24.0, Mean of 100 cons episodes: 19.15\n",
      "Episode: 219, Reward: 17.0, Mean of 100 cons episodes: 19.16\n",
      "Episode: 220, Reward: 22.0, Mean of 100 cons episodes: 19.23\n",
      "Episode: 221, Reward: 24.0, Mean of 100 cons episodes: 19.23\n",
      "Episode: 222, Reward: 32.0, Mean of 100 cons episodes: 19.12\n",
      "Episode: 223, Reward: 33.0, Mean of 100 cons episodes: 18.99\n",
      "Episode: 224, Reward: 40.0, Mean of 100 cons episodes: 18.95\n",
      "Episode: 225, Reward: 36.0, Mean of 100 cons episodes: 18.99\n",
      "Episode: 226, Reward: 22.0, Mean of 100 cons episodes: 18.92\n",
      "Episode: 227, Reward: 19.0, Mean of 100 cons episodes: 17.84\n",
      "Episode: 228, Reward: 13.0, Mean of 100 cons episodes: 16.6\n",
      "Episode: 229, Reward: 12.0, Mean of 100 cons episodes: 16.53\n",
      "Episode: 230, Reward: 14.0, Mean of 100 cons episodes: 16.37\n",
      "Episode: 231, Reward: 13.0, Mean of 100 cons episodes: 16.23\n",
      "Episode: 232, Reward: 17.0, Mean of 100 cons episodes: 16.09\n",
      "Episode: 233, Reward: 22.0, Mean of 100 cons episodes: 15.88\n",
      "Episode: 234, Reward: 23.0, Mean of 100 cons episodes: 15.2\n",
      "Episode: 235, Reward: 22.0, Mean of 100 cons episodes: 15.14\n",
      "Episode: 236, Reward: 34.0, Mean of 100 cons episodes: 15.22\n",
      "Episode: 237, Reward: 31.0, Mean of 100 cons episodes: 15.34\n",
      "Episode: 238, Reward: 60.0, Mean of 100 cons episodes: 15.59\n",
      "Episode: 239, Reward: 56.0, Mean of 100 cons episodes: 15.8\n",
      "Episode: 240, Reward: 71.0, Mean of 100 cons episodes: 16.3\n",
      "Episode: 241, Reward: 58.0, Mean of 100 cons episodes: 16.77\n",
      "Episode: 242, Reward: 58.0, Mean of 100 cons episodes: 17.39\n",
      "Episode: 243, Reward: 65.0, Mean of 100 cons episodes: 17.87\n",
      "Episode: 244, Reward: 69.0, Mean of 100 cons episodes: 18.35\n",
      "Episode: 245, Reward: 88.0, Mean of 100 cons episodes: 18.9\n",
      "Episode: 246, Reward: 35.0, Mean of 100 cons episodes: 19.38\n",
      "Episode: 247, Reward: 16.0, Mean of 100 cons episodes: 20.15\n",
      "Episode: 248, Reward: 14.0, Mean of 100 cons episodes: 20.41\n",
      "Episode: 249, Reward: 13.0, Mean of 100 cons episodes: 20.47\n",
      "Episode: 250, Reward: 14.0, Mean of 100 cons episodes: 20.52\n",
      "Episode: 251, Reward: 13.0, Mean of 100 cons episodes: 20.54\n",
      "Episode: 252, Reward: 19.0, Mean of 100 cons episodes: 20.58\n",
      "Episode: 253, Reward: 19.0, Mean of 100 cons episodes: 20.62\n",
      "Episode: 254, Reward: 38.0, Mean of 100 cons episodes: 20.71\n",
      "Episode: 255, Reward: 64.0, Mean of 100 cons episodes: 20.81\n",
      "Episode: 256, Reward: 130.0, Mean of 100 cons episodes: 21.08\n",
      "Episode: 257, Reward: 200.0, Mean of 100 cons episodes: 21.63\n",
      "Episode: 258, Reward: 200.0, Mean of 100 cons episodes: 22.84\n",
      "Episode: 259, Reward: 200.0, Mean of 100 cons episodes: 24.75\n",
      "Episode: 260, Reward: 137.0, Mean of 100 cons episodes: 26.66\n",
      "Episode: 261, Reward: 102.0, Mean of 100 cons episodes: 28.55\n",
      "Episode: 262, Reward: 84.0, Mean of 100 cons episodes: 29.83\n",
      "Episode: 263, Reward: 77.0, Mean of 100 cons episodes: 30.76\n",
      "Episode: 264, Reward: 200.0, Mean of 100 cons episodes: 31.49\n",
      "Episode: 265, Reward: 142.0, Mean of 100 cons episodes: 32.17\n",
      "Episode: 266, Reward: 139.0, Mean of 100 cons episodes: 34.07\n",
      "Episode: 267, Reward: 138.0, Mean of 100 cons episodes: 35.25\n",
      "Episode: 268, Reward: 200.0, Mean of 100 cons episodes: 36.54\n",
      "Episode: 269, Reward: 200.0, Mean of 100 cons episodes: 37.81\n",
      "Episode: 270, Reward: 77.0, Mean of 100 cons episodes: 39.71\n",
      "Episode: 271, Reward: 118.0, Mean of 100 cons episodes: 41.62\n",
      "Episode: 272, Reward: 87.0, Mean of 100 cons episodes: 42.28\n",
      "Episode: 273, Reward: 175.0, Mean of 100 cons episodes: 43.36\n",
      "Episode: 274, Reward: 62.0, Mean of 100 cons episodes: 44.07\n",
      "Episode: 275, Reward: 33.0, Mean of 100 cons episodes: 45.61\n",
      "Episode: 276, Reward: 14.0, Mean of 100 cons episodes: 44.23\n",
      "Episode: 277, Reward: 16.0, Mean of 100 cons episodes: 44.39\n",
      "Episode: 278, Reward: 15.0, Mean of 100 cons episodes: 44.4\n",
      "Episode: 279, Reward: 12.0, Mean of 100 cons episodes: 44.47\n",
      "Episode: 280, Reward: 13.0, Mean of 100 cons episodes: 44.52\n",
      "Episode: 281, Reward: 12.0, Mean of 100 cons episodes: 44.56\n",
      "Episode: 282, Reward: 17.0, Mean of 100 cons episodes: 44.58\n",
      "Episode: 283, Reward: 51.0, Mean of 100 cons episodes: 44.6\n",
      "Episode: 284, Reward: 200.0, Mean of 100 cons episodes: 44.67\n",
      "Episode: 285, Reward: 139.0, Mean of 100 cons episodes: 45.09\n",
      "Episode: 286, Reward: 112.0, Mean of 100 cons episodes: 47.01\n",
      "Episode: 287, Reward: 106.0, Mean of 100 cons episodes: 48.31\n",
      "Episode: 288, Reward: 120.0, Mean of 100 cons episodes: 49.34\n",
      "Episode: 289, Reward: 200.0, Mean of 100 cons episodes: 50.3\n",
      "Episode: 290, Reward: 200.0, Mean of 100 cons episodes: 51.4\n",
      "Episode: 291, Reward: 200.0, Mean of 100 cons episodes: 53.3\n",
      "Episode: 292, Reward: 200.0, Mean of 100 cons episodes: 55.2\n",
      "Episode: 293, Reward: 200.0, Mean of 100 cons episodes: 57.09\n",
      "Episode: 294, Reward: 134.0, Mean of 100 cons episodes: 58.99\n",
      "Episode: 295, Reward: 71.0, Mean of 100 cons episodes: 60.91\n",
      "Episode: 296, Reward: 33.0, Mean of 100 cons episodes: 62.16\n",
      "Episode: 297, Reward: 54.0, Mean of 100 cons episodes: 62.78\n",
      "Episode: 298, Reward: 47.0, Mean of 100 cons episodes: 63.02\n",
      "Episode: 299, Reward: 37.0, Mean of 100 cons episodes: 63.45\n",
      "Episode: 300, Reward: 43.0, Mean of 100 cons episodes: 63.81\n",
      "Episode: 301, Reward: 74.0, Mean of 100 cons episodes: 64.07\n",
      "Episode: 302, Reward: 64.0, Mean of 100 cons episodes: 64.38\n",
      "Episode: 303, Reward: 81.0, Mean of 100 cons episodes: 65.01\n",
      "Episode: 304, Reward: 146.0, Mean of 100 cons episodes: 65.53\n",
      "Episode: 305, Reward: 150.0, Mean of 100 cons episodes: 66.23\n",
      "Episode: 306, Reward: 200.0, Mean of 100 cons episodes: 67.56\n",
      "Episode: 307, Reward: 200.0, Mean of 100 cons episodes: 68.93\n",
      "Episode: 308, Reward: 200.0, Mean of 100 cons episodes: 70.79\n",
      "Episode: 309, Reward: 200.0, Mean of 100 cons episodes: 72.67\n",
      "Episode: 310, Reward: 200.0, Mean of 100 cons episodes: 74.52\n",
      "Episode: 311, Reward: 200.0, Mean of 100 cons episodes: 76.4\n",
      "Episode: 312, Reward: 200.0, Mean of 100 cons episodes: 78.24\n",
      "Episode: 313, Reward: 200.0, Mean of 100 cons episodes: 80.06\n",
      "Episode: 314, Reward: 200.0, Mean of 100 cons episodes: 81.91\n",
      "Episode: 315, Reward: 200.0, Mean of 100 cons episodes: 83.77\n",
      "Episode: 316, Reward: 200.0, Mean of 100 cons episodes: 85.6\n",
      "Episode: 317, Reward: 200.0, Mean of 100 cons episodes: 87.39\n",
      "Episode: 318, Reward: 200.0, Mean of 100 cons episodes: 89.24\n",
      "Episode: 319, Reward: 191.0, Mean of 100 cons episodes: 91.02\n",
      "Episode: 320, Reward: 191.0, Mean of 100 cons episodes: 92.78\n",
      "Episode: 321, Reward: 179.0, Mean of 100 cons episodes: 94.52\n",
      "Episode: 322, Reward: 150.0, Mean of 100 cons episodes: 96.21\n",
      "Episode: 323, Reward: 150.0, Mean of 100 cons episodes: 97.76\n",
      "Episode: 324, Reward: 156.0, Mean of 100 cons episodes: 98.94\n",
      "Episode: 325, Reward: 161.0, Mean of 100 cons episodes: 100.11\n",
      "Episode: 326, Reward: 138.0, Mean of 100 cons episodes: 101.27\n",
      "Episode: 327, Reward: 161.0, Mean of 100 cons episodes: 102.52\n",
      "Episode: 328, Reward: 155.0, Mean of 100 cons episodes: 103.68\n",
      "Episode: 329, Reward: 170.0, Mean of 100 cons episodes: 105.1\n",
      "Episode: 330, Reward: 182.0, Mean of 100 cons episodes: 106.52\n",
      "Episode: 331, Reward: 173.0, Mean of 100 cons episodes: 108.1\n",
      "Episode: 332, Reward: 154.0, Mean of 100 cons episodes: 109.78\n",
      "Episode: 333, Reward: 130.0, Mean of 100 cons episodes: 111.38\n",
      "Episode: 334, Reward: 124.0, Mean of 100 cons episodes: 112.75\n",
      "Episode: 335, Reward: 118.0, Mean of 100 cons episodes: 113.83\n",
      "Episode: 336, Reward: 97.0, Mean of 100 cons episodes: 114.84\n",
      "Episode: 337, Reward: 78.0, Mean of 100 cons episodes: 115.8\n",
      "Episode: 338, Reward: 60.0, Mean of 100 cons episodes: 116.43\n",
      "Episode: 339, Reward: 56.0, Mean of 100 cons episodes: 116.9\n",
      "Episode: 340, Reward: 24.0, Mean of 100 cons episodes: 116.9\n",
      "Episode: 341, Reward: 56.0, Mean of 100 cons episodes: 116.9\n",
      "Episode: 342, Reward: 66.0, Mean of 100 cons episodes: 116.43\n",
      "Episode: 343, Reward: 83.0, Mean of 100 cons episodes: 116.41\n",
      "Episode: 344, Reward: 97.0, Mean of 100 cons episodes: 116.49\n",
      "Episode: 345, Reward: 129.0, Mean of 100 cons episodes: 116.67\n",
      "Episode: 346, Reward: 185.0, Mean of 100 cons episodes: 116.95\n",
      "Episode: 347, Reward: 175.0, Mean of 100 cons episodes: 117.36\n",
      "Episode: 348, Reward: 200.0, Mean of 100 cons episodes: 118.86\n",
      "Episode: 349, Reward: 131.0, Mean of 100 cons episodes: 120.45\n",
      "Episode: 350, Reward: 127.0, Mean of 100 cons episodes: 122.31\n",
      "Episode: 351, Reward: 127.0, Mean of 100 cons episodes: 123.49\n",
      "Episode: 352, Reward: 162.0, Mean of 100 cons episodes: 124.62\n",
      "Episode: 353, Reward: 200.0, Mean of 100 cons episodes: 125.76\n",
      "Episode: 354, Reward: 134.0, Mean of 100 cons episodes: 127.19\n",
      "Episode: 355, Reward: 50.0, Mean of 100 cons episodes: 129.0\n",
      "Episode: 356, Reward: 115.0, Mean of 100 cons episodes: 129.96\n",
      "Episode: 357, Reward: 200.0, Mean of 100 cons episodes: 129.82\n",
      "Episode: 358, Reward: 200.0, Mean of 100 cons episodes: 129.67\n",
      "Episode: 359, Reward: 200.0, Mean of 100 cons episodes: 129.67\n",
      "Episode: 360, Reward: 191.0, Mean of 100 cons episodes: 129.67\n",
      "Episode: 361, Reward: 177.0, Mean of 100 cons episodes: 129.67\n",
      "Episode: 362, Reward: 200.0, Mean of 100 cons episodes: 130.21\n",
      "Episode: 363, Reward: 200.0, Mean of 100 cons episodes: 130.96\n",
      "Episode: 364, Reward: 200.0, Mean of 100 cons episodes: 132.12\n",
      "Episode: 365, Reward: 200.0, Mean of 100 cons episodes: 133.35\n",
      "Episode: 366, Reward: 200.0, Mean of 100 cons episodes: 133.35\n",
      "Episode: 367, Reward: 200.0, Mean of 100 cons episodes: 133.93\n",
      "Episode: 368, Reward: 200.0, Mean of 100 cons episodes: 134.54\n",
      "Episode: 369, Reward: 162.0, Mean of 100 cons episodes: 135.16\n",
      "Episode: 370, Reward: 133.0, Mean of 100 cons episodes: 135.16\n",
      "Episode: 371, Reward: 115.0, Mean of 100 cons episodes: 134.78\n",
      "Episode: 372, Reward: 112.0, Mean of 100 cons episodes: 135.34\n",
      "Episode: 373, Reward: 117.0, Mean of 100 cons episodes: 135.31\n",
      "Episode: 374, Reward: 129.0, Mean of 100 cons episodes: 135.56\n",
      "Episode: 375, Reward: 165.0, Mean of 100 cons episodes: 134.98\n",
      "Episode: 376, Reward: 200.0, Mean of 100 cons episodes: 135.65\n",
      "Episode: 377, Reward: 200.0, Mean of 100 cons episodes: 136.97\n",
      "Episode: 378, Reward: 171.0, Mean of 100 cons episodes: 138.83\n",
      "Episode: 379, Reward: 200.0, Mean of 100 cons episodes: 140.67\n",
      "Episode: 380, Reward: 200.0, Mean of 100 cons episodes: 142.23\n",
      "Episode: 381, Reward: 200.0, Mean of 100 cons episodes: 144.11\n",
      "Episode: 382, Reward: 200.0, Mean of 100 cons episodes: 145.98\n",
      "Episode: 383, Reward: 200.0, Mean of 100 cons episodes: 147.86\n",
      "Episode: 384, Reward: 200.0, Mean of 100 cons episodes: 149.69\n",
      "Episode: 385, Reward: 200.0, Mean of 100 cons episodes: 151.18\n",
      "Episode: 386, Reward: 200.0, Mean of 100 cons episodes: 151.18\n",
      "Episode: 387, Reward: 200.0, Mean of 100 cons episodes: 151.79\n",
      "Episode: 388, Reward: 200.0, Mean of 100 cons episodes: 152.67\n",
      "Episode: 389, Reward: 200.0, Mean of 100 cons episodes: 153.61\n",
      "Episode: 390, Reward: 200.0, Mean of 100 cons episodes: 154.41\n",
      "Episode: 391, Reward: 200.0, Mean of 100 cons episodes: 154.41\n",
      "Episode: 392, Reward: 200.0, Mean of 100 cons episodes: 154.41\n",
      "Episode: 393, Reward: 200.0, Mean of 100 cons episodes: 154.41\n",
      "Episode: 394, Reward: 200.0, Mean of 100 cons episodes: 154.41\n",
      "Episode: 395, Reward: 200.0, Mean of 100 cons episodes: 154.41\n",
      "Episode: 396, Reward: 200.0, Mean of 100 cons episodes: 155.07\n",
      "Episode: 397, Reward: 200.0, Mean of 100 cons episodes: 156.36\n",
      "Episode: 398, Reward: 200.0, Mean of 100 cons episodes: 158.03\n",
      "Episode: 399, Reward: 200.0, Mean of 100 cons episodes: 159.49\n",
      "Episode: 400, Reward: 200.0, Mean of 100 cons episodes: 161.02\n",
      "Episode: 401, Reward: 200.0, Mean of 100 cons episodes: 162.65\n",
      "Episode: 402, Reward: 200.0, Mean of 100 cons episodes: 164.22\n",
      "Episode: 403, Reward: 200.0, Mean of 100 cons episodes: 165.48\n",
      "Episode: 404, Reward: 200.0, Mean of 100 cons episodes: 166.84\n",
      "Episode: 405, Reward: 173.0, Mean of 100 cons episodes: 168.03\n",
      "Episode: 406, Reward: 172.0, Mean of 100 cons episodes: 168.57\n",
      "Episode: 407, Reward: 197.0, Mean of 100 cons episodes: 168.8\n",
      "Episode: 408, Reward: 200.0, Mean of 100 cons episodes: 168.52\n",
      "Episode: 409, Reward: 183.0, Mean of 100 cons episodes: 168.49\n",
      "Episode: 410, Reward: 108.0, Mean of 100 cons episodes: 168.49\n",
      "Episode: 411, Reward: 194.0, Mean of 100 cons episodes: 168.32\n",
      "Episode: 412, Reward: 200.0, Mean of 100 cons episodes: 167.4\n",
      "Episode: 413, Reward: 200.0, Mean of 100 cons episodes: 167.34\n",
      "Episode: 414, Reward: 190.0, Mean of 100 cons episodes: 167.34\n",
      "Episode: 415, Reward: 200.0, Mean of 100 cons episodes: 167.34\n",
      "Episode: 416, Reward: 200.0, Mean of 100 cons episodes: 167.24\n",
      "Episode: 417, Reward: 200.0, Mean of 100 cons episodes: 167.24\n",
      "Episode: 418, Reward: 200.0, Mean of 100 cons episodes: 167.24\n",
      "Episode: 419, Reward: 200.0, Mean of 100 cons episodes: 167.24\n",
      "Episode: 420, Reward: 200.0, Mean of 100 cons episodes: 167.24\n",
      "Episode: 421, Reward: 200.0, Mean of 100 cons episodes: 167.33\n",
      "Episode: 422, Reward: 200.0, Mean of 100 cons episodes: 167.42\n",
      "Episode: 423, Reward: 200.0, Mean of 100 cons episodes: 167.63\n",
      "Episode: 424, Reward: 200.0, Mean of 100 cons episodes: 168.13\n",
      "Episode: 425, Reward: 200.0, Mean of 100 cons episodes: 168.63\n",
      "Episode: 426, Reward: 200.0, Mean of 100 cons episodes: 169.07\n",
      "Episode: 427, Reward: 200.0, Mean of 100 cons episodes: 169.46\n",
      "Episode: 428, Reward: 200.0, Mean of 100 cons episodes: 170.08\n",
      "Episode: 429, Reward: 200.0, Mean of 100 cons episodes: 170.47\n",
      "Episode: 430, Reward: 200.0, Mean of 100 cons episodes: 170.92\n",
      "Episode: 431, Reward: 200.0, Mean of 100 cons episodes: 171.22\n",
      "Episode: 432, Reward: 200.0, Mean of 100 cons episodes: 171.4\n",
      "Episode: 433, Reward: 200.0, Mean of 100 cons episodes: 171.67\n",
      "Episode: 434, Reward: 200.0, Mean of 100 cons episodes: 172.13\n",
      "Episode: 435, Reward: 200.0, Mean of 100 cons episodes: 172.83\n",
      "Episode: 436, Reward: 200.0, Mean of 100 cons episodes: 173.59\n",
      "Episode: 437, Reward: 188.0, Mean of 100 cons episodes: 174.41\n",
      "Episode: 438, Reward: 170.0, Mean of 100 cons episodes: 175.44\n",
      "Episode: 439, Reward: 200.0, Mean of 100 cons episodes: 176.54\n",
      "Episode: 440, Reward: 200.0, Mean of 100 cons episodes: 177.64\n",
      "Episode: 441, Reward: 200.0, Mean of 100 cons episodes: 179.08\n",
      "Episode: 442, Reward: 164.0, Mean of 100 cons episodes: 180.84\n",
      "Episode: 443, Reward: 164.0, Mean of 100 cons episodes: 182.28\n",
      "Episode: 444, Reward: 199.0, Mean of 100 cons episodes: 183.26\n",
      "Episode: 445, Reward: 200.0, Mean of 100 cons episodes: 184.07\n",
      "Episode: 446, Reward: 200.0, Mean of 100 cons episodes: 185.09\n",
      "Episode: 447, Reward: 200.0, Mean of 100 cons episodes: 185.8\n",
      "Episode: 448, Reward: 200.0, Mean of 100 cons episodes: 185.95\n",
      "Episode: 449, Reward: 200.0, Mean of 100 cons episodes: 186.2\n",
      "Episode: 450, Reward: 200.0, Mean of 100 cons episodes: 186.2\n",
      "Episode: 451, Reward: 200.0, Mean of 100 cons episodes: 186.89\n",
      "Episode: 452, Reward: 193.0, Mean of 100 cons episodes: 187.62\n",
      "Episode: 453, Reward: 200.0, Mean of 100 cons episodes: 188.35\n",
      "Episode: 454, Reward: 200.0, Mean of 100 cons episodes: 188.66\n",
      "Episode: 455, Reward: 200.0, Mean of 100 cons episodes: 188.66\n",
      "Episode: 456, Reward: 200.0, Mean of 100 cons episodes: 189.32\n",
      "Episode: 457, Reward: 200.0, Mean of 100 cons episodes: 190.82\n",
      "Episode: 458, Reward: 200.0, Mean of 100 cons episodes: 191.67\n",
      "Episode: 459, Reward: 200.0, Mean of 100 cons episodes: 191.67\n",
      "Episode: 460, Reward: 200.0, Mean of 100 cons episodes: 191.67\n",
      "Episode: 461, Reward: 200.0, Mean of 100 cons episodes: 191.67\n",
      "Episode: 462, Reward: 200.0, Mean of 100 cons episodes: 191.76\n",
      "Episode: 463, Reward: 200.0, Mean of 100 cons episodes: 191.99\n",
      "Episode: 464, Reward: 200.0, Mean of 100 cons episodes: 191.99\n",
      "Episode: 465, Reward: 200.0, Mean of 100 cons episodes: 191.99\n",
      "Episode: 466, Reward: 200.0, Mean of 100 cons episodes: 191.99\n",
      "Episode: 467, Reward: 200.0, Mean of 100 cons episodes: 191.99\n",
      "Episode: 468, Reward: 197.0, Mean of 100 cons episodes: 191.99\n",
      "Episode: 469, Reward: 200.0, Mean of 100 cons episodes: 191.99\n",
      "Episode: 470, Reward: 200.0, Mean of 100 cons episodes: 191.96\n",
      "Episode: 471, Reward: 200.0, Mean of 100 cons episodes: 192.34\n",
      "Episode: 472, Reward: 200.0, Mean of 100 cons episodes: 193.01\n",
      "Episode: 473, Reward: 200.0, Mean of 100 cons episodes: 193.86\n",
      "Episode: 474, Reward: 200.0, Mean of 100 cons episodes: 194.74\n",
      "Episode: 475, Reward: 200.0, Mean of 100 cons episodes: 195.57\n",
      "Problem solved after 475 Episode with the mean reward 195.57 over the last 100 episodes \n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: /tmp/cartpole_exp1/Q_replay/agent_061120200617/assets\n",
      "\n",
      "\n",
      "Problem is solved after 475 Episode with the mean reward 195.57 over the last 100 episodes\n"
     ]
    }
   ],
   "source": [
    "# Running the algorithm for a maximum number of iteration until it is solved\n",
    "tot_rews = []\n",
    "mean_100ep = 0.0\n",
    "for episode in range(agent_par['num_episodes']):\n",
    "\n",
    "    # Do one rollout. When remember=True, data is saved in policy.memory\n",
    "    _, _, rewards, _, _ = CP.one_rollout(policy, remember=True)\n",
    "\n",
    "    # Update the network using experience replay\n",
    "    loss = policy.replay(agent_par['batch_size'])\n",
    "\n",
    "    # Check if the problem is solved\n",
    "    if episode > 100:\n",
    "        mean_100ep = np.mean(tot_rews[-101:-1])\n",
    "\n",
    "    tot_reward = sum(rewards)\n",
    "    tot_rews.append(tot_reward)\n",
    "    print(f\"Episode: {episode}, Reward: {tot_reward}, Mean of 100 cons episodes: {mean_100ep}\")\n",
    "    if mean_100ep > env_par['threshold']:\n",
    "        print(f\"Problem solved after {episode} Episode with the mean reward {mean_100ep} over the last 100 episodes \")\n",
    "        policy.network.save(agent_path)\n",
    "        break\n",
    "\n",
    "    # Save data\n",
    "    with train_writer.as_default():\n",
    "        tf.summary.scalar('reward', tot_reward, step=episode)\n",
    "\n",
    "# Close the environment\n",
    "CP.env.close()\n",
    "\n",
    "# Print the summary of the solution\n",
    "if mean_100ep > env_par['threshold']:\n",
    "    print(f\"\\n\\nProblem is solved after {episode} Episode with the mean reward {mean_100ep} over the last 100 episodes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVs2_pPgMiUJ"
   },
   "source": [
    "## Results\n",
    "It will get around a minute to run the above cell. You will probably get some WARNING\\ERROR. Some of these are related to incompatibility between some libraries. Don't panic. The problem will be solved after possibly after 475-500 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xrjaw2OMMiUK"
   },
   "source": [
    "It is a quite good improvement. It is good to know that [the best algorithm](https://gym.openai.com/evaluations/eval_lEi8I8v2QLqEgzBxcvRIaA/) solves this problem after around 211 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIEMLoOVMiUL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of replay_q_on_cartpole_notebook.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/FarnazAdib/Crash_course_on_RL/blob/master/replay_q_on_cartpole_notebook.ipynb",
     "timestamp": 1604643495477
    }
   ]
  },
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
